{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== Anlage des Spark Context ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spark-Session erstellen und notwendige Frameworks importieren**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/01/16 22:57:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "#conf.set(\"spark.driver.bindAddress\", \"127.0.0.1\")  # Localhost\n",
    "#conf = conf.setMaster(\"local[*]\")\n",
    "conf = conf.setAppName(\"App\")\n",
    "\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except Exception as e:\n",
    "    print(f\"Fehler:  {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Falls Spark-Session mit Fehler startet -> stoppen\n",
    "# sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== **Dateipfade festlegen und allgemeine Variable** ====\n",
    "\n",
    "- `airport_data_path`: Pfad zur CSV-Datei mit Flughafendaten (`airports.csv`).\n",
    "- `aircraft_data_path`: Pfad zur CSV-Datei mit vollständigen Flugzeugdaten (`aircraft-database-complete-2024-10.csv`).\n",
    "- `flight_data_path`: Pfad zur CSV-Datei mit vorverarbeiteten Flugbewegungsdaten (`processed_data_reduced.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data_path = \"data/airports.csv\"\n",
    "aircraft_data_path = 'data/aircraft-database-complete-2024-10.csv'\n",
    "flight_data_path = 'data/processed_data_reduced.csv'\n",
    "\n",
    "dquote='\\\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== **Vorverarbeitung der Datenquelle** ===\n",
    "- Nur notwendig für Vorverarbeitung von Bewegungsdaten zur Umwandlung in notwendiges Datenformat\n",
    "- Laden: Liest eine Pipe-getrennte CSV-Datei (`data.csv`).\n",
    "- Spalten auswählen und umbenennen**: Wählt die Spalten `icao24`, `time`, `latitude`, `longitude`, `onground` aus und benennt sie um.\n",
    "- Daten konvertieren**:\n",
    "    - Zeit: Wandelt Unix-Timestamps in lesbares Format um.\n",
    "    - Status: Konvertiert `onground` von `true`/`false` zu `1`/`0`.\n",
    "- Speichern: Speichert die bereinigten Daten in `processed_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from datetime import datetime\n",
    "\n",
    "# CSV-Datei laden, wobei das Pipe-Zeichen als Trennzeichen angegeben wird\n",
    "#csv_file_path = \"data.csv\"  # Ersetze dies mit dem Pfad zu deiner CSV-Datei\n",
    "#df = pd.read_csv(csv_file_path, sep='|')  # Verwende '|' als Trennzeichen\n",
    "\n",
    "# Angenommene Spaltennummern: time = 0, icao24 = 1, lat = 2, lon = 3, onground = 4\n",
    "#column_indices = [1, 0, 2, 3, 4]  # Die Spalten in der gewünschten Reihenfolge auswählen\n",
    "#df_selected = df.iloc[:, column_indices]\n",
    "\n",
    "# Umbenennen der Spalten\n",
    "#df_selected.columns = ['time', 'icao24', 'latitude', 'longitude', 'onground']\n",
    "\n",
    "# Funktion zur Konvertierung des Unix-Timestamps\n",
    "#def convert_timestamp(unix_timestamp):\n",
    "#    try:\n",
    "#        return datetime.utcfromtimestamp(int(unix_timestamp)).strftime('%Y/%m/%d %H:%M:%S')\n",
    "#    except ValueError:\n",
    "#        return None\n",
    "\n",
    "# Konvertiere die 'time'-Spalte\n",
    "#df_selected['time'] = df_selected['time'].apply(convert_timestamp)\n",
    "\n",
    "# Umwandlung der 'onground'-Spalte, 'true' -> 1 und 'false' -> 0\n",
    "#df_selected['onground'] = df_selected['onground'].apply(lambda x: 1 if str(x).strip().lower() == 'true' else 0)\n",
    "\n",
    "# Sicherstellen, dass 'time' an erster Stelle bleibt\n",
    "#df_selected = df_selected[['time', 'icao24', 'latitude', 'longitude', 'onground']]\n",
    "\n",
    "# Speichere das Ergebnis als neue CSV-Datei\n",
    "#df_selected.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POC zu Parquet**\n",
    "\n",
    "- Liest ein Parquet-File (`bigDataSet_v2.parquet`) in einen Spark DataFrame.  \n",
    "- Zeigt die ersten 3 Zeilen (`df.show(3)`) und druckt Schema und Spaltennamen.  \n",
    "- Wandelt den DataFrame in ein RDD um (`df.rdd`) und gibt die ersten 3 Einträge aus (`rdd.take(3)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code wie man das Parquet File anlegt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Inspect Parquet\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.parquet(\"data/bigDataSet_v2.parquet\")\n",
    "df.show(3)  # Zeigt die ersten 5 Zeilen der Parquet-Datei\n",
    "\n",
    "df.printSchema()\n",
    "print(\"Spaltennamen:\", df.columns)\n",
    "\n",
    "# Umwandeln des DataFrames in ein RDD\n",
    "rdd = df.rdd\n",
    "\n",
    "# Ausgabe der ersten 3 Zeilen\n",
    "print(rdd.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== **Durchführen der Datenanlyse** ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auswertung 1 - Weltweite Flughäfen**\n",
    "\n",
    "- Liest eine CSV-Datei mit weltweiten Flughafendaten ein und verarbeitet sie als RDD.  \n",
    "- Filtert nach ausgewählten Spalten, entfernt den Header und parst die Daten korrekt als CSV.  \n",
    "- Bestimmt die Anzahl aller relevanten Flughäfen/Heliports und zählt diese nach Typ.  \n",
    "- Gibt einen formatierten Ergebnis-String und das RDD zurück.\n",
    "\n",
    "Skalierbarkeit:\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem -> Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten -> Einfach zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Zählungen im Speicher -> Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimenisonierung eines realen Systems\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def analyze_airports(airport_data_path):\n",
    "    \"\"\"\n",
    "    Analysiert Flughäfen weltweit und gibt die Anzahl als String zurück.\n",
    "    \n",
    "    :return: Formatierten String mit den Ergebnissen und das zugehörige RDD.\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten einlesen und Header extrahieren\n",
    "    airportRDD = sc.textFile(airport_data_path)\n",
    "    header = airportRDD.first()\n",
    "\n",
    "    # CSV korrekt parsen, um Fehler durch Kommata innerhalb von Anführungszeichen zu vermeiden\n",
    "    def parse_csv(line):\n",
    "        reader = csv.reader(StringIO(line))\n",
    "        return next(reader)\n",
    "\n",
    "    # Filterkriterien anwenden und nur die relevanten Spalten speichern (1, 2, 3, 4, 5, 7)\n",
    "    airportRDD = (\n",
    "        airportRDD.filter(lambda line: line != header)  # Header entfernen\n",
    "                  .map(parse_csv)  # CSV korrekt parsen\n",
    "                  .filter(lambda cols: len(cols) > 7)  # Sicherstellen, dass genug Spalten vorhanden sind\n",
    "                  .map(lambda cols: (cols[1].strip('\"'),  # Name\n",
    "                                    cols[2].strip('\"'),  # Typ\n",
    "                                    cols[3].strip('\"'),  # Breitengrad\n",
    "                                    cols[4].strip('\"'),  # Längengrad\n",
    "                                    cols[5].strip('\"'),  # Höhe\n",
    "                                    #cols[7].strip('\"')\n",
    "                                    ))  # Kontinent\n",
    "                  .filter(lambda cols: cols[1] in [\"small_airport\", \"medium_airport\", \"large_airport\", \"heliport\"])  # Nur bestimmte Typen\n",
    "    )\n",
    "\n",
    "    # Anzahl der Flughäfen insgesamt\n",
    "    total_airports = airportRDD.count()\n",
    "\n",
    "    # Anzahl der Flughäfen nach Kategorien\n",
    "    categorized_airports = (\n",
    "        airportRDD.map(lambda cols: cols[1])  # Typ (Spalte 2)\n",
    "                  .countByValue()\n",
    "    )\n",
    "\n",
    "    # Formatierte Ausgabe als String erstellen\n",
    "    result = \"Analyse weltweiter Flughäfen und Heliports\\n\"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "    result += f\"Die Anzahl der weltweiten Flughäfen und Heliports beträgt: {total_airports}\\n\"\n",
    "    result += f\"Davon Heliports: {categorized_airports.get('heliport', 0)}\\n\"\n",
    "    result += f\"Davon kleine Flughäfen: {categorized_airports.get('small_airport', 0)}\\n\"\n",
    "    result += f\"Davon mittelgroße Flughäfen: {categorized_airports.get('medium_airport', 0)}\\n\"\n",
    "    result += f\"Davon große Flughäfen: {categorized_airports.get('large_airport', 0)}\\n\"\n",
    "\n",
    "    return result, airportRDD\n",
    "\n",
    "# Beispielaufruf und Ausgabe\n",
    "result, airportRDD = analyze_airports(airport_data_path)\n",
    "print(result)\n",
    "\n",
    "# Optional: Die Daten aus airportRDD anzeigen lassen\n",
    "#airportRDD.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airportRDD.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auswertung 2 - Registrierte Flugobjekte**\n",
    "\n",
    "Liest eine CSV-Datei mit Flugzeugdaten und entfernt den Header.\n",
    "- Zählt die Anzahl registrierter Flugobjekte.\n",
    "- Ermittelt die Top 20 Airlines nach Anzahl der registrierten Flugzeuge.\n",
    "- Gibt einen formatierten String und zwei RDDs zurück (gefilterte Daten und Airlines mit Anzahl).\n",
    "\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Einfach zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_aircraft_data(aircraft_data_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Analysiert eine CSV-Datei mit Flugzeugdaten, um die Gesamtanzahl der registrierten Flugobjekte \n",
    "    sowie die Top 20 Airlines nach der Anzahl der registrierten Flugzeuge zu ermitteln.\n",
    "\n",
    "    Die Funktion liest die Daten aus einer festgelegten CSV-Datei, filtert ungültige Einträge \n",
    "    und verarbeitet die Informationen mithilfe von Spark. \n",
    "\n",
    "    :return: Ein Tupel bestehend aus:\n",
    "        - result (str): Ein formatierter String mit der Gesamtanzahl der Flugobjekte \n",
    "                        und den Top 20 Airlines nach Anzahl registrierter Flugzeuge.\n",
    "        - aircraft_raw_rdd (RDD): Ein Spark RDD mit den gefilterten Flugzeugdaten.\n",
    "        - aircraftRDD (RDD): Ein Spark RDD, das die Airlines und die Anzahl ihrer Flugzeuge enthält.\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten einlesen und Header extrahieren\n",
    "    aircraft_input_rdd = sc.textFile(aircraft_data_path)\n",
    "    header = aircraft_input_rdd.first()\n",
    "\n",
    "    # Filterkriterien anwenden: Entfernen des Headers\n",
    "    aircraft_raw_rdd = aircraft_input_rdd.filter(lambda line: line != header)\n",
    "    \n",
    "    # Anzahl der registrierten Flugobjekte\n",
    "    Anzahl_registrierte_Flugobjekte = aircraft_raw_rdd.count()\n",
    "\n",
    "    # RDD erstellen, um die Airlines nach Anzahl der registrierten Flugzeuge zu zählen\n",
    "    aircraftRDD = (\n",
    "        aircraft_raw_rdd.filter(lambda line: len(line.split(\",\")) > 16)\n",
    "        .map(lambda line: line.split(\",\")[17].strip('\"'))\n",
    "        .filter(lambda airline: airline != \"''\")\n",
    "        .map(lambda airline: (airline, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "    )\n",
    "    \n",
    "    # Top 20 Airlines nach Anzahl der registrierten Flugzeuge\n",
    "    top_20_airlines = aircraftRDD.take(20)\n",
    "\n",
    "    # Ergebnis als formatierten String zurückgeben\n",
    "    result = f\"Anzahl der weltweit gemeldeten, eindeutigen Flug- und Bodenobjekte: {Anzahl_registrierte_Flugobjekte}\\n\"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "    result += \"Top 20 Airlines nach Anzahl der registrierten Flugzeuge:\\n\"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "    for airline, count in top_20_airlines:\n",
    "        result += f\"{airline}: {count}\\n\"\n",
    "\n",
    "    return result, aircraft_raw_rdd, aircraftRDD\n",
    "\n",
    "# Beispielaufruf und Ausgabe\n",
    "result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auswertung 3 - Weltweite Bewegungsdaten von Flugobjekten in einer Woche**\n",
    "\n",
    "- Liest Flugbewegungsdaten ein, entfernt Header und ungültige Einträge.\n",
    "- Nutzt einen einstellbaren Prozentsatz der Daten (`sampleRDD`).\n",
    "- Bestimmt den zeitlichen Rahmen (erstes und letztes Datum) sowie min./max. Längen- und Breitengrad.\n",
    "- Berechnet das Zentrum und erzeugt eine Folium-Karte mit einem Rechteck des betrachteten Luftraums.\n",
    "- Gibt einen Ergebnis-Text, die Karte und das gefilterte RDD zurück.\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Folium: Erstellen und Rendern der Karte erfolgt auf dem Driver, bei großen geografischen Bereichen viel Speicher und CPU\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Teilweise Ausfälle beeinflussen nicht den Prozess\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung der Visualisierung bspw. durch separate Ressourcen für die Generierung der Folium-Karte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die erfassten Bewegungen von Flug- und Bodenobjekten wurden im Zeitraum vom 2024/12/24 08:00:00 bis zum 2024/12/25 16:59:59 (Zeitzone: UTC) ermittelt.\n",
      "==================================================\n",
      "Geografische Abdeckung:\n",
      "Längengrade: von -157.8611° bis 175.1362°\n",
      "Breitengrade: von -46.1685° bis 68.8769°\n",
      "Die Daten wurden in folgendem Luftraum erhoben.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"width:100%;\"><div style=\"position:relative;width:100%;height:0;padding-bottom:60%;\"><span style=\"color:#565656\">Make this Notebook Trusted to load map: File -> Trust Notebook</span><iframe srcdoc=\"&lt;!DOCTYPE html&gt;\n",
       "&lt;html&gt;\n",
       "&lt;head&gt;\n",
       "    \n",
       "    &lt;meta http-equiv=&quot;content-type&quot; content=&quot;text/html; charset=UTF-8&quot; /&gt;\n",
       "    \n",
       "        &lt;script&gt;\n",
       "            L_NO_TOUCH = false;\n",
       "            L_DISABLE_3D = false;\n",
       "        &lt;/script&gt;\n",
       "    \n",
       "    &lt;style&gt;html, body {width: 100%;height: 100%;margin: 0;padding: 0;}&lt;/style&gt;\n",
       "    &lt;style&gt;#map {position:absolute;top:0;bottom:0;right:0;left:0;}&lt;/style&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://code.jquery.com/jquery-3.7.1.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/js/bootstrap.bundle.min.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;script src=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js&quot;&gt;&lt;/script&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/leaflet@1.9.3/dist/leaflet.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/bootstrap@5.2.2/dist/css/bootstrap.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://netdna.bootstrapcdn.com/bootstrap/3.0.0/css/bootstrap-glyphicons.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.2.0/css/all.min.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css&quot;/&gt;\n",
       "    &lt;link rel=&quot;stylesheet&quot; href=&quot;https://cdn.jsdelivr.net/gh/python-visualization/folium/folium/templates/leaflet.awesome.rotate.min.css&quot;/&gt;\n",
       "    \n",
       "            &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width,\n",
       "                initial-scale=1.0, maximum-scale=1.0, user-scalable=no&quot; /&gt;\n",
       "            &lt;style&gt;\n",
       "                #map_a6b4e98dd8ce357bc916742529010bc1 {\n",
       "                    position: relative;\n",
       "                    width: 100.0%;\n",
       "                    height: 100.0%;\n",
       "                    left: 0.0%;\n",
       "                    top: 0.0%;\n",
       "                }\n",
       "                .leaflet-container { font-size: 1rem; }\n",
       "            &lt;/style&gt;\n",
       "        \n",
       "&lt;/head&gt;\n",
       "&lt;body&gt;\n",
       "    \n",
       "    \n",
       "            &lt;div class=&quot;folium-map&quot; id=&quot;map_a6b4e98dd8ce357bc916742529010bc1&quot; &gt;&lt;/div&gt;\n",
       "        \n",
       "&lt;/body&gt;\n",
       "&lt;script&gt;\n",
       "    \n",
       "    \n",
       "            var map_a6b4e98dd8ce357bc916742529010bc1 = L.map(\n",
       "                &quot;map_a6b4e98dd8ce357bc916742529010bc1&quot;,\n",
       "                {\n",
       "                    center: [11.354192733764648, 8.637552897135421],\n",
       "                    crs: L.CRS.EPSG3857,\n",
       "                    zoom: 5,\n",
       "                    zoomControl: true,\n",
       "                    preferCanvas: false,\n",
       "                }\n",
       "            );\n",
       "\n",
       "            \n",
       "\n",
       "        \n",
       "    \n",
       "            var tile_layer_a95f1455d3a42fe4d7d733624674742f = L.tileLayer(\n",
       "                &quot;https://tile.openstreetmap.org/{z}/{x}/{y}.png&quot;,\n",
       "                {&quot;attribution&quot;: &quot;\\u0026copy; \\u003ca href=\\&quot;https://www.openstreetmap.org/copyright\\&quot;\\u003eOpenStreetMap\\u003c/a\\u003e contributors&quot;, &quot;detectRetina&quot;: false, &quot;maxNativeZoom&quot;: 19, &quot;maxZoom&quot;: 19, &quot;minZoom&quot;: 0, &quot;noWrap&quot;: false, &quot;opacity&quot;: 1, &quot;subdomains&quot;: &quot;abc&quot;, &quot;tms&quot;: false}\n",
       "            );\n",
       "        \n",
       "    \n",
       "            tile_layer_a95f1455d3a42fe4d7d733624674742f.addTo(map_a6b4e98dd8ce357bc916742529010bc1);\n",
       "        \n",
       "    \n",
       "            var rectangle_63b55eb625dbda4d24c74410f68c6a2a = L.rectangle(\n",
       "                [[-46.168521881103516, -157.86112467447916], [68.87690734863281, 175.13623046875]],\n",
       "                {&quot;bubblingMouseEvents&quot;: true, &quot;color&quot;: &quot;red&quot;, &quot;dashArray&quot;: null, &quot;dashOffset&quot;: null, &quot;fill&quot;: true, &quot;fillColor&quot;: &quot;blue&quot;, &quot;fillOpacity&quot;: 0.2, &quot;fillRule&quot;: &quot;evenodd&quot;, &quot;lineCap&quot;: &quot;round&quot;, &quot;lineJoin&quot;: &quot;round&quot;, &quot;noClip&quot;: false, &quot;opacity&quot;: 1.0, &quot;smoothFactor&quot;: 1.0, &quot;stroke&quot;: true, &quot;weight&quot;: 2}\n",
       "            ).addTo(map_a6b4e98dd8ce357bc916742529010bc1);\n",
       "        \n",
       "    \n",
       "        var popup_464529d554eba8b15835b899d2eb0629 = L.popup({&quot;maxWidth&quot;: &quot;100%&quot;});\n",
       "\n",
       "        \n",
       "            \n",
       "                var html_86ac55566677253001aa6d7b91df0794 = $(`&lt;div id=&quot;html_86ac55566677253001aa6d7b91df0794&quot; style=&quot;width: 100.0%; height: 100.0%;&quot;&gt;Geographischer Bereich&lt;/div&gt;`)[0];\n",
       "                popup_464529d554eba8b15835b899d2eb0629.setContent(html_86ac55566677253001aa6d7b91df0794);\n",
       "            \n",
       "        \n",
       "\n",
       "        rectangle_63b55eb625dbda4d24c74410f68c6a2a.bindPopup(popup_464529d554eba8b15835b899d2eb0629)\n",
       "        ;\n",
       "\n",
       "        \n",
       "    \n",
       "&lt;/script&gt;\n",
       "&lt;/html&gt;\" style=\"position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;\" allowfullscreen webkitallowfullscreen mozallowfullscreen></iframe></div></div>"
      ],
      "text/plain": [
       "<folium.folium.Map at 0x1240d8980>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import folium\n",
    "\n",
    "# Analyse der Bewegungsdaten\n",
    "def analyze_flight_movements(flight_data_path, percentage=1.0):\n",
    "    \"\"\"\n",
    "    Analysiert Bewegungsdaten von Flugzeugen und erstellt eine Karte des geografischen Bereichs.\n",
    "    Misst dabei die Laufzeit für die Verarbeitung des Prozentsatzes der Daten.\n",
    "    \n",
    "    :param percentage: Prozentsatz (zwischen 0 und 1) der Daten, die geladen werden sollen (Standard: 100%)\n",
    "    :return: Die Ergebnisse der Auswertung, generierte Karte und das RDD der Daten sowie die benötigte Zeit\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 < percentage <= 1:\n",
    "        raise ValueError(\"Percentage muss zwischen 0 und 1 liegen\")\n",
    "    \n",
    "    # Einlesen der Bewegungsdaten eines definierten Zeitraums in einem bestimmten geografischen Bereich\n",
    "    myFileRDD = sc.textFile(flight_data_path)\n",
    "    header = myFileRDD.take(1)\n",
    "    sampleRDD_with_null = myFileRDD.filter(lambda line: line not in header)\n",
    "    sampleRDD = sampleRDD_with_null.filter(lambda line: \"NULL\" not in line)\n",
    "\n",
    "    #Reduktion des RDDs auf Prozentwert an Datensätzen, seed sorgt für reproduzierbare Zufallsergebnisse (42 ist die Antwort auf alles).\n",
    "    dataRDD = sampleRDD.sample(False, percentage, seed=42)\n",
    "\n",
    "    #Erfassen des Zeitraums in dem Daten die erhoben wurden - Skalierbarkeit gegeben aufgrund der Rückgabe eines Wertes\n",
    "    start_time = dataRDD.map(lambda line: line.split(',')[0]).min()\n",
    "    end_time = dataRDD.map(lambda line: line.split(',')[0]).max()\n",
    "\n",
    "    #Geographische Analyse des betrachteten Luftraums - Skalierbarkeit gegeben aufgrund der Rückgabe und Visualisierung einzelner Werte\n",
    "\n",
    "    # Längengrade sortieren und auslesen\n",
    "    min_longitude = dataRDD.map(lambda line: float(line.split(',')[3])).min()\n",
    "    max_longitude = dataRDD.map(lambda line: float(line.split(',')[3])).max()\n",
    "\n",
    "    # Breitengrade (vermutlich Spalte mit Index 5 und 6)\n",
    "    min_latitude = dataRDD.map(lambda line: float(line.split(',')[2])).min()\n",
    "    max_latitude = dataRDD.map(lambda line: float(line.split(',')[2])).max()\n",
    "\n",
    "    # Zentrum des Bereichs berechnen\n",
    "    center_lat = (min_latitude + max_latitude) / 2\n",
    "    center_lon = (min_longitude + max_longitude) / 2\n",
    "    result_text = (\n",
    "        f\"Die erfassten Bewegungen von Flug- und Bodenobjekten wurden im Zeitraum vom \"\n",
    "        f\"{start_time.replace(dquote, '').replace('+00', '')} bis zum \"\n",
    "        f\"{end_time.replace(dquote, '').replace('+00', '')} (Zeitzone: UTC) ermittelt.\\n\"\n",
    "        f\"{'=' * 50}\\n\"\n",
    "        \"Geografische Abdeckung:\\n\"\n",
    "        f\"Längengrade: von {min_longitude:.4f}° bis {max_longitude:.4f}°\\n\"\n",
    "        f\"Breitengrade: von {min_latitude:.4f}° bis {max_latitude:.4f}°\\n\"\n",
    "        \"Die Daten wurden in folgendem Luftraum erhoben.\"\n",
    "    )\n",
    "\n",
    "    # Karte erstellen\n",
    "    map = folium.Map(location=[center_lat, center_lon], zoom_start=5)\n",
    "\n",
    "    # Rechteck auf der Karte hinzufügen\n",
    "    bounds = [[min_latitude, min_longitude], [max_latitude, max_longitude]]\n",
    "    folium.Rectangle(\n",
    "        bounds=bounds,\n",
    "        color=\"red\",\n",
    "        weight=2,\n",
    "        fill=True,\n",
    "        fill_color=\"blue\",\n",
    "        fill_opacity=0.2,\n",
    "        popup=\"Geographischer Bereich\"\n",
    "    ).add_to(map)\n",
    "\n",
    "    return result_text, map, dataRDD,  \n",
    "\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "print(result_text)\n",
    "map_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auswertung 4 - Kombination Flugobjekte und Bewegungsdaten**\n",
    "\n",
    "- Liest Bewegungsdaten (Zeitstempel, Koordinaten, Flugzeug-ID etc.) ein.  \n",
    "- Parsed jeden Datensatz zu einem Key-Value-Paar (`icao24` als Schlüssel und weitere Daten als Dictionary).  \n",
    "- Gruppiert danach alle Einträge mit derselben Flugzeug-ID und gibt ein RDD zurück, in dem jeder Schlüssel einer Liste seiner Bewegungsdaten zugeordnet ist.\n",
    "\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Teilweise Ausfälle beeinflussen nicht den Prozess\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung des Speichers: genügend Speicher für Join-Operationen und Aggregationen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kombination der Bewegungsdaten\n",
    "\n",
    "- Parst das `aircraft_raw_rdd`, um zu jedem Flugzeug die zugehörigen Informationen (Hersteller, Modell, Airline usw.) zu extrahieren.  \n",
    "- Verknüpft diese Informationen mithilfe einer `leftOuterJoin` mit den bereits gruppierten Bewegungsdaten (`vehicle_rdd`).  \n",
    "- Gibt ein kombiniertes RDD zurück, in dem für jede Flugzeug-ID sowohl die sortierten Bewegungsdaten als auch die Flugzeuginformationen hinterlegt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kombination der Bewegungsdaten mit den Daten der registrierten Flug- und Bodenobjekten\n",
    "\n",
    "def combine_rdds(vehicle_rdd, aircraft_raw_rdd):\n",
    "    \"\"\"\n",
    "    Kombiniert die Bewegungsdaten gruppiert nach Fahrzeug über icao (vehicle_rdd) mit den Daten der registrierten Flug- und Bodenobjekte (aircraft_raw_rdd).\n",
    "\n",
    "    Args:\n",
    "        vehicle_rdd: Ein RDD mit Bewegungsdaten.\n",
    "        aircraft_raw_rdd: Ein RDD mit Rohdaten zu registrierten Flug- und Bodenobjekten.\n",
    "\n",
    "    Returns:\n",
    "        Ein kombiniertes RDD mit Flugzeug-ID, Flugdaten und Beschreibungen.\n",
    "    \"\"\"\n",
    "    def parse_type(line):\n",
    "        parts = line.split(',')\n",
    "        # Baseline-Verarbeitung ohne unnötige Komplexität\n",
    "        return (\n",
    "            parts[0].strip(\"'\"),  # Flugzeug-ID (icao24)\n",
    "            {\n",
    "                \"description\": parts[5].strip(\"'\") if len(parts) > 4 else \"\",\n",
    "                \"manufacturer\": parts[13].strip(\"'\") if len(parts) > 12 else \"\",\n",
    "                \"model\": parts[14].strip(\"'\") if len(parts) > 14 else \"\",\n",
    "                \"type\": parts[15].strip(\"'\") if len(parts) > 15 else \"\",\n",
    "                \"airline\": parts[18].strip(\"'\") if len(parts) > 16 else \"\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # RDD bereinigen und nur relevante Daten aufnehmen\n",
    "    cleaned_type_rdd = aircraft_raw_rdd.map(parse_type)\n",
    "\n",
    "    # Kombination der Bewegungsdaten mit der Beschreibung der registrierten Flug- und Bodenobjekten\n",
    "    result_rdd = vehicle_rdd.leftOuterJoin(cleaned_type_rdd)\n",
    "\n",
    "    # Kombination der Daten in einem RDD\n",
    "    combined_rdd = result_rdd.map(lambda x: (\n",
    "        x[0],  # Flugzeug-ID\n",
    "        {\n",
    "            \"aircraft_info\": x[1][1] if x[1][1] else {},  # Beschreibung, falls vorhanden\n",
    "            \"flight_data\": sorted(x[1][0], key=lambda entry: entry[\"time\"]) if x[1][0] else [] # Flugdaten sortiert nach Datum (lexografisch)\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    return combined_rdd\n",
    "\n",
    "\n",
    "combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "#combined_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des zuvor kombinierten RDD\n",
    "\n",
    "- Nimmt ein kombiniertes RDD (Flugzeug- und Bewegungsdaten) und wendet eine Stichprobenziehung an (standardmäßig 100%).  \n",
    "- Bestimmt die Gesamtzahl aktiver Flug- und Bodenobjekte, listet die Top 5 mit den meisten Datensätzen und zeigt an, wie viele gerade fliegen bzw. am Boden sind.  \n",
    "- Analysiert außerdem, ob Objekte im gesamten Zeitraum nur am Boden, nur in der Luft oder teils geflogen sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse des kombinierten RDDs (Bewegungsdaten + Beschreibung)\n",
    "\n",
    "def analyze_combined_rdd(sampleRDD, percentage=1.0):\n",
    "    \"\"\"\n",
    "    Analysiert ein kombiniertes RDD, das Bewegungsdaten von Flug- und Bodenobjekten enthält,\n",
    "    und liefert eine detaillierte statistische Zusammenfassung der betrachteten Daten.\n",
    "\n",
    "    Args:\n",
    "    sampleRDD (RDD): Ein RDD, das Flug- und Bewegungsdaten von Objekten enthält, wobei jedes Element ein Tupel ist,\n",
    "                      wobei das erste Element die Objekt-ID ist und das zweite Element ein Dictionary mit den Bewegungsdaten.\n",
    "    percentage (float): Der Anteil der Stichprobe (zwischen 0 und 1) der RDD-Daten, die für die Analyse verwendet werden sollen. \n",
    "                        Standardwert ist 1.0 (100%).\n",
    "\n",
    "    Returns:\n",
    "    str: Eine formatierte Zeichenkette, die die Ergebnisse der Analyse zusammenfasst, einschließlich der Gesamtzahl der\n",
    "         Objekte, der Flugzeuge mit den meisten Datensätzen und der Status- und Bewegungsanalyse für den betrachteten Zeitraum.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Reduktion des RDDs auf Prozentwert an Datensätzen, seed sorgt für reproduzierbare Zufallsergebnisse (42 ist die Antwort auf alles).\n",
    "    combined_rdd = sampleRDD.sample(False, percentage, seed=42)\n",
    "    amount_of_aircrafts = combined_rdd.count()\n",
    "    result = \"=\" * 50 + \"\\n\"\n",
    "    result += f\"Anzahl der im betrachteten Zeitraum aktiven Flug- und Bodenobjekte: {amount_of_aircrafts}\\n\"\n",
    "    \n",
    "    #Ermittlung der Anzahl an Flug-Datensätze pro Flugzeug\n",
    "    countdatasetsRDD = combined_rdd.map(lambda x: (x[0],len(x[1]['flight_data']))).sortBy(lambda x: x[1], ascending=False)\n",
    "    first_5_elements = countdatasetsRDD.take(5)\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "    result += f\"Flugzeuge mit den meisten Datensätzen (Top 5) sind: {first_5_elements}\\n \"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "\n",
    "    # Bestimmung der Positon - Skalierbarkeit gegeben über Betrachtung einzelner Werte\n",
    "    airRDD = combined_rdd.map(lambda x: (x[0], x[1]['flight_data'][-1][\"onground\"]))\n",
    "    onground = airRDD.filter(lambda x: x[1]==1).count()\n",
    "    inair = airRDD.filter(lambda x: x[1]==0).count()\n",
    "    result += f\"Status der Objekt zum letzten gemessenen Zeitpunkt: \\n\"\n",
    "    result += f\"Objekte aktuell in der Luft: {inair}\\n\"\n",
    "    result += f\"Objekte aktuell am Boden: {onground}\\n\"\n",
    "\n",
    "\n",
    "    # Ermittlung der Position im Zeitraum - Skalierbarkeit gegeben über Betrachtung einzelner Werte\n",
    "    nextRDD = combined_rdd.map(lambda x: (x[0], sum(datapoint[\"onground\"] for datapoint in x[1]['flight_data']), len(x[1]['flight_data'])))\n",
    "    calculatedRDD = nextRDD.map(lambda x: (x[0], x[1]/x[2]))\n",
    "    startet_or_landed = calculatedRDD.filter(lambda x: x[1] < 1 and x[1] >0).count()\n",
    "    not_started = calculatedRDD.filter(lambda x: x[1] == 1).count()\n",
    "    nlRDD = calculatedRDD.filter(lambda x: x[1] == 0)\n",
    "    not_landed = calculatedRDD.filter(lambda x: x[1] == 0).count()\n",
    "\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "    result += f\"Im gesamten betrachteten Zeitraum sind: \\n\"\n",
    "    result += f\"Objekte nur am Boden geblieben: {not_started}\\n\"\n",
    "    result += f\"Objekte nur in der Luft geblieben: {not_landed}\\n\"\n",
    "    result += f\"Objekte gestartet oder gelandet: {startet_or_landed}\"\n",
    "\n",
    " \n",
    "    return combined_rdd, result\n",
    "\n",
    "combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== **Visuelle Analysen** ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifizierung von Starts und Landungen von Flugobjekten und Verknüpfung mit nahe gelegenen Flughäfen (innerhalb von 10 km)**\n",
    "\n",
    "- Berechnet mithilfe der Haversine-Formel die Distanz zwischen zwei Koordinatenpunkten.  \n",
    "- Erkennt Start- und Landemomente anhand von `onground`-Werten (`1 -> 0` und zurück).  \n",
    "- Sucht zu jedem Start- und Landezeitpunkt den nächsten Flughafen (max. 10 km entfernt).  \n",
    "- Ignoriert Datensätze von Oberflächenfahrzeugen (Surface Vehicles).  \n",
    "- Gibt eine Liste mit allen erkannten Flügen inklusive Start-/Endkoordinaten und nächstgelegenen Flughäfen zurück.\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Teilweise Ausfälle beeinflussen nicht den Prozess\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung des Speichers: genügend Speicher für Join-Operationen und Aggregationen\n",
    "    - Optimierung der Visualisierung: separate Ressourcen für die Generierung der Folium-Karte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Haversine-Funktion zum Berechnen der Distanz zwischen zwei Punkten\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371  # Erd-Radius in Kilometern\n",
    "    d_lat = math.radians(lat2 - lat1)\n",
    "    d_lon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(d_lat / 2.0) ** 2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(d_lon / 2.0) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def find_nearest_airport(lat, lon, airports):\n",
    "    \"\"\"\n",
    "    Findet den nächsten Flughafen zu gegebenen Koordinaten innerhalb von 10 km.\n",
    "    Gibt den Flughafen mit der geringsten Distanz zurück.\n",
    "    \"\"\"\n",
    "    nearest_airport = None\n",
    "    min_distance = float(\"inf\")\n",
    "\n",
    "    for airport in airports:\n",
    "        airport_id, _, airport_name, airport_lat, airport_lon = airport\n",
    "        airport_lat = float(airport_lat)\n",
    "        airport_lon = float(airport_lon)\n",
    "        distance = haversine(lat, lon, airport_lat, airport_lon)\n",
    "        if distance <= 10 and distance < min_distance:  # Innerhalb von 10 km und geringste Distanz\n",
    "            min_distance = distance\n",
    "            nearest_airport = (airport_id, airport_name)\n",
    "\n",
    "    if nearest_airport:\n",
    "        return nearest_airport\n",
    "    return None, None\n",
    "\n",
    "def extract_flights_with_airports(record, airports):\n",
    "    \"\"\"\n",
    "    Extrahiert Flüge und fügt Flughafendaten hinzu.\n",
    "    \"\"\"\n",
    "    icao24, combined_rdd = record\n",
    "    aircraft_info = combined_rdd['aircraft_info']\n",
    "    flight_data = combined_rdd['flight_data']\n",
    "\n",
    "    # Filter: Ignoriere Datensätze mit \"Surface Vehicle\" in der Beschreibung\n",
    "    if 'description' in aircraft_info and 'Surface Vehicle' in aircraft_info['description']:\n",
    "        return []\n",
    "\n",
    "    flights = []\n",
    "    current_flight = []\n",
    "    in_flight = False\n",
    "    previous_entry = None  # Variable, um den vorherigen Datensatz zu speichern\n",
    "\n",
    "    for entry in flight_data:\n",
    "        # Überprüfung, ob es sich um einen Start handelt\n",
    "        if not in_flight and entry['onground'] == 0 and previous_entry and previous_entry['onground'] == 1:\n",
    "            in_flight = True\n",
    "            current_flight = [entry]  # Startpunkt hinzufügen\n",
    "        elif in_flight and entry['onground'] == 1:  # Landung erkannt\n",
    "            current_flight.append(entry)  # Landepunkt hinzufügen\n",
    "            # Flug-Informationen extrahieren\n",
    "            start = current_flight[0]\n",
    "            end = current_flight[-1]\n",
    "\n",
    "            start_airport_id, start_airport_name = find_nearest_airport(start['lat'], start['lon'], airports)\n",
    "            end_airport_id, end_airport_name = find_nearest_airport(end['lat'], end['lon'], airports)\n",
    "\n",
    "            flight_info = {\n",
    "                'start_time': start['time'],\n",
    "                'start_coords': (start['lat'], start['lon']),\n",
    "                'start_airport': {'id': start_airport_id, 'name': start_airport_name},\n",
    "                'end_time': end['time'],\n",
    "                'end_coords': (end['lat'], end['lon']),\n",
    "                'end_airport': {'id': end_airport_id, 'name': end_airport_name},\n",
    "            }\n",
    "            flights.append((icao24, aircraft_info, flight_info))\n",
    "            in_flight = False\n",
    "            current_flight = []\n",
    "        elif in_flight:  # Punkte während des Fluges sammeln\n",
    "            current_flight.append(entry)\n",
    "\n",
    "        # Speichere den aktuellen Eintrag als vorherigen für die nächste Iteration\n",
    "        previous_entry = entry\n",
    "\n",
    "    return flights\n",
    "\n",
    "# Airports-Daten als Liste sammeln (Broadcast-Variable verwenden)\n",
    "# Annahme: airportRDD ist das gegebene RDD mit den Flughafendaten\n",
    "airports_list = airportRDD.collect()\n",
    "broadcast_airports = sc.broadcast(airports_list)\n",
    "\n",
    "# Anwenden der Extraktionsfunktion auf die RDD mit Flughafendaten\n",
    "flights_rdd = combined_rdd.flatMap(lambda record: extract_flights_with_airports(record, broadcast_airports.value))\n",
    "\n",
    "# Sammle alle Daten aus dem RDD\n",
    "all_flights = flights_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- Aggregiert Flüge anhand ihrer Start- und Endflughäfen und zählt die Verbindungen.  \n",
    "- Erstellt daraus ein Netzwerkdiagramm (Graph) mit networkX und filtert nur Verbindungen mit mehr als 5 Flügen.  \n",
    "- Berechnet die Knotenpositionen per `spring_layout` und visualisiert das Ganze interaktiv in Plotly.\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Begrenzt zu skalieren, da alle Daten müssen auf den Driver-Knoten geladen werden müssen\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Networkx / Plotty profitieren nicht von den Fehlertoleranzmechanismen\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung des Speichers: genügend Speicher für Join-Operationen und Aggregationen\n",
    "    - Alternativen für große Netzwerke: verteilte Graphenverarbeitungstools wie GraphX oder GraphFrames für verbesserte Skalierbarkeit/Fehlertoleranz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Beispiel-Daten erweitern: Liste von Flügen\n",
    "flights_data = flights_rdd.collect()\n",
    "\n",
    "# Schritt 1: Flugverbindungen aggregieren (Start- und End-Airport-Paare zählen)\n",
    "connections = defaultdict(int)\n",
    "start_end_names = {}\n",
    "\n",
    "for _, _, flight_info in flights_data:\n",
    "    start_airport = flight_info['start_airport']\n",
    "    end_airport = flight_info['end_airport']\n",
    "    \n",
    "    if start_airport['id'] and end_airport['id']:  # Nur valide Airports berücksichtigen\n",
    "        start_id, start_name = start_airport['id'], start_airport['name']\n",
    "        end_id, end_name = end_airport['id'], end_airport['name']\n",
    "        \n",
    "        connections[(start_id, end_id)] += 1\n",
    "        start_end_names[start_id] = start_name\n",
    "        start_end_names[end_id] = end_name\n",
    "\n",
    "# Schritt 2: Netzwerkdiagramm erstellen\n",
    "G = nx.Graph()\n",
    "\n",
    "# Knoten und Kanten hinzufügen (nur Verbindungen mit mehr als 5 Flügen)\n",
    "for (start, end), weight in connections.items():\n",
    "    if weight > 5:  # Filter: Nur Verbindungen > 5\n",
    "        G.add_edge(start, end, weight=weight)\n",
    "\n",
    "# Schritt 3: Positionen berechnen\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Knoten und Kanten für Plotly vorbereiten\n",
    "edges_x = []\n",
    "edges_y = []\n",
    "weights = []\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edges_x.extend([x0, x1, None])\n",
    "    edges_y.extend([y0, y1, None])\n",
    "    weights.append(edge[2]['weight'])\n",
    "\n",
    "# Knoten vorbereiten\n",
    "nodes_x = []\n",
    "nodes_y = []\n",
    "node_text = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    nodes_x.append(x)\n",
    "    nodes_y.append(y)\n",
    "    node_text.append(start_end_names.get(node, node))  # Namen verwenden\n",
    "\n",
    "# Schritt 4: Interaktives Diagramm mit Plotly erstellen\n",
    "fig = go.Figure()\n",
    "\n",
    "# Kanten hinzufügen mit stärkerer Gewichtung\n",
    "max_weight = max(weights) if weights else 1  # Maximale Gewichtung für Skalierung\n",
    "\n",
    "for i, edge in enumerate(G.edges(data=True)):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x0, x1, None],\n",
    "        y=[y0, y1, None],\n",
    "        line=dict(width=(weights[i] / max_weight) * 100, color='black'),  # Stärkere Skalierung (100-fach)\n",
    "        hoverinfo='none',\n",
    "        mode='lines'))\n",
    "\n",
    "# Knoten hinzufügen\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=nodes_x, y=nodes_y,\n",
    "    mode='markers+text',\n",
    "    text=node_text,\n",
    "    textposition=\"top center\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color='skyblue',\n",
    "        line_width=2),\n",
    "    hoverinfo='text'))\n",
    "\n",
    "# Layout anpassen\n",
    "fig.update_layout(\n",
    "    title=\"Interaktives Netzwerkdiagramm der Flugverbindungen (Verbindungen > 5)\",\n",
    "    title_x=0.5,\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    margin=dict(b=0, l=0, r=0, t=30),\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Interaktives Diagramm anzeigen\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisierung der Bewegung und Beschreibung eines ausgewählten Flug- oder Bodenobjektes**\n",
    "\n",
    "- Wählt mittels einer spezifischen Flugzeug-ID (`selected_icao`) die zugehörigen Flugdaten aus.  \n",
    "- Erzeugt eine Folium-Karte, die die Route zwischen aufeinanderfolgenden Punkten als Linie darstellt.  \n",
    "- Markiert Landepunkte (dort, wo `onground == 1`) mit einem grünen Marker und passenden Informationen (Hersteller, Modell, Airline etc.).\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Begrenzt zu skalieren, da alle Daten auf Driver-Knoten (collect()) geladen werden müssen\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Begrenzt zu skalieren, Berechnen der PolyLines erfordert CPU-Ressourcen auf dem Driver.\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Folium läuft auf dem Drive Knoten ist nicht fehlertolerant\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen, gerade Speicher und CPU Auslastung auf Drive Knoten\n",
    "    - Optimierung der Visualisierung: auf separate Prozesse ausgelagert werden, um die Performance des Driver-Knotens zu verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import display\n",
    "\n",
    "def analyze_aircraft_route(selected_icao):\n",
    "    # Filtere die Daten für die spezifische Flugzeug-ID im RDD\n",
    "    filtered_data = combined_rdd.filter(lambda x: x[0] == selected_icao).collect()\n",
    "\n",
    "    if filtered_data:\n",
    "        # Hole die Daten für das ausgewählte Flugzeug\n",
    "        icao24, route_data = filtered_data[0]\n",
    "        aircraft_info = route_data[\"aircraft_info\"]  # Zugriff auf die Flugzeugbeschreibung\n",
    "        flight_data = route_data[\"flight_data\"]  # Zugriff auf die Flugdaten\n",
    "\n",
    "        print(f\"Flugzeug-ID: {icao24}\")\n",
    "\n",
    "        # Erstelle die Karte, zentriert auf den ersten Datenpunkt\n",
    "        first_lat = flight_data[0][\"lat\"]\n",
    "        first_lon = flight_data[0][\"lon\"]\n",
    "        map = folium.Map(location=[first_lat, first_lon], zoom_start=12)\n",
    "\n",
    "        # Zeichne Linien zwischen aufeinanderfolgenden Punkten\n",
    "        for i in range(len(flight_data) - 1):\n",
    "            point_a = flight_data[i]\n",
    "            point_b = flight_data[i + 1]\n",
    "\n",
    "            # Zeichne nur Linien, wenn beide Punkte gültige Koordinaten haben\n",
    "            if point_a[\"lat\"] and point_a[\"lon\"] and point_b[\"lat\"] and point_b[\"lon\"]:\n",
    "                folium.PolyLine(\n",
    "                    locations=[(point_a[\"lat\"], point_a[\"lon\"]), (point_b[\"lat\"], point_b[\"lon\"])],\n",
    "                    color=\"blue\",\n",
    "                    weight=2.5,\n",
    "                    opacity=0.8\n",
    "                ).add_to(map)\n",
    "\n",
    "        # Füge Marker nur für Landepunkte hinzu\n",
    "        for point in flight_data:\n",
    "            if point[\"onground\"] == 1:  # Nur Landepunkte anzeigen\n",
    "                folium.Marker(\n",
    "                    location=(point[\"lat\"], point[\"lon\"]),\n",
    "                    popup=f\"Zeit: {point['time']}<br>Am Boden: {point['onground']}<br>Hersteller: {aircraft_info['manufacturer']}<br>Flugzeugtyp: {aircraft_info['model']}<br>Airline: {aircraft_info['airline']}\",\n",
    "                    icon=folium.Icon(color=\"green\", icon=\"info-sign\")\n",
    "                ).add_to(map)\n",
    "\n",
    "        # Rückgabe der Karte\n",
    "        return map\n",
    "\n",
    "    else:\n",
    "        return f\"Keine Daten für Flugzeug-ID '{selected_icao}' gefunden.\"\n",
    "\n",
    "\n",
    "result = analyze_aircraft_route(\"a1cf48\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisierung der Bewegung und Beschreibung mehrerer, zufällig ausgewählter Flug- oder Bodenobjekte**\n",
    "\n",
    "- Wählt zufällig eine bestimmte Anzahl an Flugzeug-Routen aus (`number_of_samples`) und zeichnet sie auf einer Folium-Karte.  \n",
    "- Pro Flugzeug wird die Route zwischen aufeinanderfolgenden Punkten in einer zufällig generierten Farbe eingezeichnet.  \n",
    "- Landepunkte (``onground == 1``) werden als grüne Marker mit Informationen über Hersteller, Modell, Airline etc. versehen.  \n",
    "- Die Karte wird schließlich zentriert auf den ersten Punkt des ersten Flugzeugs.\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Begrenzt zu skalieren, da alle Daten auf Driver-Knoten geladen werden müssen\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Begrenzt zu skalieren, Berechnen der PolyLines erfordert CPU-Ressourcen auf dem Driver.\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Folium läuft auf dem Drive Knoten ist nicht fehlertolerant\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen, gerade Speicher und CPU Auslastung auf Drive Knoten\n",
    "    - Optimierung der Visualisierung: auf separate Prozesse ausgelagert werden, um die Performance des Driver-Knotens zu verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "# Zufällige Farben für die Flugzeuge\n",
    "def get_random_color():\n",
    "    return \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "def analyze_aircraft_routes(number_of_samples):\n",
    "    # Nimm eine beliebige Anzahl zufälliger Flugzeuge aus dem RDD\n",
    "    random_aircrafts = combined_rdd.takeSample(withReplacement=False, num=number_of_samples)\n",
    "\n",
    "    # Erstelle eine Karte, zentriert auf einen Beispielpunkt\n",
    "    if random_aircrafts:\n",
    "        # Hole den ersten Punkt zur Zentrierung der Karte\n",
    "        first_lat = random_aircrafts[0][1][\"flight_data\"][0][\"lat\"]\n",
    "        first_lon = random_aircrafts[0][1][\"flight_data\"][0][\"lon\"]\n",
    "        map = folium.Map(location=[first_lat, first_lon], zoom_start=6)\n",
    "\n",
    "        # Iteriere über die zufälligen Flugzeuge\n",
    "        for icao24, data in random_aircrafts:\n",
    "            aircraft_info = data[\"aircraft_info\"]  # Infos über das Flugzeug\n",
    "            flight_data = data[\"flight_data\"]      # Flugdaten\n",
    "\n",
    "            # Sichere Abfrage von Typ, Modell und Hersteller mit Standardwerten\n",
    "            aircraft_type = aircraft_info.get(\"type\", \"Unknown\")\n",
    "            aircraft_model = aircraft_info.get(\"model\", \"Unknown\")\n",
    "            aircraft_manufacturer = aircraft_info.get(\"manufacturer\", \"Unknown\")\n",
    "            aircraft_airline = aircraft_info.get(\"airline\", \"Unknown\")\n",
    "\n",
    "            # Generiere eine zufällige Farbe für die Linien\n",
    "            color = get_random_color()\n",
    "\n",
    "            # Zeichne Linien zwischen aufeinanderfolgenden Punkten\n",
    "            for i in range(len(flight_data) - 1):\n",
    "                point_a = flight_data[i]\n",
    "                point_b = flight_data[i + 1]\n",
    "\n",
    "                # Zeichne nur Linien, wenn beide Punkte gültige Koordinaten haben\n",
    "                if point_a[\"lat\"] and point_a[\"lon\"] and point_b[\"lat\"] and point_b[\"lon\"]:\n",
    "                    folium.PolyLine(\n",
    "                        locations=[(point_a[\"lat\"], point_a[\"lon\"]), (point_b[\"lat\"], point_b[\"lon\"])],\n",
    "                        color=color,\n",
    "                        weight=2.5,\n",
    "                        opacity=0.8\n",
    "                    ).add_to(map)\n",
    "\n",
    "            # Füge Marker nur für Landepunkte hinzu\n",
    "            for point in flight_data:\n",
    "                if point[\"onground\"] == 1:  # Nur Landepunkte anzeigen\n",
    "                    folium.Marker(\n",
    "                        location=(point[\"lat\"], point[\"lon\"]),\n",
    "                        popup=(\n",
    "                            f\"Flugzeug: {icao24}<br>\"\n",
    "                            f\"Hersteller: {aircraft_manufacturer}<br>\"\n",
    "                            f\"Modell: {aircraft_model}<br>\"\n",
    "                            f\"Airline: {aircraft_airline}<br>\"\n",
    "                            f\"Zeit: {point['time']}<br>\"\n",
    "                            f\"Am Boden: {point['onground']}\"\n",
    "                        ),\n",
    "                        icon=folium.Icon(color=\"green\", icon=\"info-sign\")\n",
    "                    ).add_to(map)\n",
    "\n",
    "        # Zeige die Karte im Notebook\n",
    "        return map\n",
    "    else:\n",
    "        return \"Keine Daten gefunden.\"\n",
    "    \n",
    "result = analyze_aircraft_routes(1)\n",
    "display(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Heatmap zur visuellen Auswertung aller Bewegungsdaten**\n",
    "\n",
    "- Extrahiert Koordinaten (Breiten- und Längengrad)** aus dem Eingabe-RDD und rundet sie auf eine bestimmte Genauigkeit (hier `round(...)` mit einer Nachkommastelle).  \n",
    "- Aggregiert die Häufigkeiten jeder Koordinatenkombination, um beispielsweise Cluster zu identifizieren.  \n",
    "- Erstellt abschließend eine Heatmap mit Folium basierend auf den gezählten Häufigkeiten.\n",
    "\n",
    "Skalierbarkeit\n",
    "- Verarbeitungslaufzeit wächst proportional zur Datenmenge (Horzizontale Skalierung von Spark wird durch die Darstellung der Headmap limitiert)\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Gut skalierbar durch paralleles Einlesen und Verarbeiten auf mehreren Knoten\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Begrenzt zu skalieren, Berechnen der PolyLines erfordert CPU-Ressourcen auf dem Driver.\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Eingeschränkt zu skalieren \n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Folium läuft auf dem Drive Knoten ist nicht fehlertolerant\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen, gerade Speicher und CPU Auslastung auf Drive Knoten\n",
    "    - Optimierung der Visualisierung: auf separate Prozesse ausgelagert werden, um die Performance des Driver-Knotens zu verbessern -> Datenmenge begrenzen, weitere Filterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "def process_and_reduce(dataRDD, percentage=1.0):\n",
    "    \"\"\"\n",
    "    Verarbeitet das Eingabe-RDD, extrahiert die Koordinaten (Breitengrad, Längengrad) \n",
    "    und reduziert die Daten, indem die Häufigkeit jedes Koordinatenpaars gezählt wird.\n",
    "    Gibt das reduzierte RDD der Koordinaten und deren Häufigkeiten zurück.\n",
    "\n",
    "    # Map und Filter: Direktes Überprüfen der Konvertierbarkeit in float\n",
    "    # 1 Dezimalstelle: ~11,1 km Genauigkeit eine Nachkommastelle reicht für globale Karten oder grobe Cluster.\n",
    "    # 2 Dezimalstellen: ~1,1 km Genauigkeit gut für Städte oder Regionen.\n",
    "    # 3 Dezimalstellen: ~110 m Genauigkeit ideal für Stadtviertel oder grobe Stadtanalysen.\n",
    "\n",
    "    \"\"\"\n",
    "    # Extrahiere Koordinaten\n",
    "    def extract_coordinates(line):\n",
    "        try:\n",
    "            parts = line.split(',')\n",
    "            return round(float(parts[2]), 1), round(float(parts[3]), 1)  # Breitengrad, Längengrad\n",
    "        except (ValueError, IndexError):\n",
    "            return None  # Falls Konvertierung oder Zugriff fehlschlägt\n",
    "\n",
    "    # Reduktion der Daten\n",
    "    cleaned_rdd = dataRDD.map(extract_coordinates).filter(lambda x: x is not None)\n",
    "\n",
    "    # Zähle, wie oft jede Kombination von Breitengrad und Längengrad vorkommt\n",
    "    reduced_rdd = cleaned_rdd.map(lambda coord: (coord, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    result = f\"Anzahl der Daten vor der Reduktion: {dataRDD.count()}\\n\"\n",
    "    result += f\"Anzahl Koordinaten nach der Reduktion: {reduced_rdd.count()}\\n\"\n",
    "\n",
    "    return reduced_rdd, result\n",
    "\n",
    "\n",
    "def create_heatmap(reduced_rdd):\n",
    "    \"\"\"\n",
    "    Erstellt eine Heatmap aus den aggregierten Koordinaten und deren Häufigkeiten.\n",
    "    \"\"\"\n",
    "\n",
    "    # Beispiel: Aggregierte Koordinaten mit Häufigkeiten (ersetzt durch deine Daten)\n",
    "    reduced_data = reduced_rdd.collect()\n",
    "\n",
    "    # Vorbereitung der Heatmap-Daten\n",
    "    heatmap_data = [(lat_lon[0], lat_lon[1], count) for lat_lon, count in reduced_data]\n",
    "\n",
    "    # Erstelle die Karte\n",
    "    if heatmap_data:\n",
    "        # Zentriere die Karte auf den ersten Punkt und passe die Zoom-Stufe an\n",
    "        map = folium.Map(location=[heatmap_data[0][0], heatmap_data[0][1]], zoom_start=6)\n",
    "\n",
    "        # HeatMap hinzufügen (Radius und max_zoom können angepasst werden)\n",
    "        HeatMap(heatmap_data, radius=15, max_zoom=13).add_to(map)\n",
    "        \n",
    "        return map\n",
    "    \n",
    "    else:\n",
    "        print(\"Keine gültigen Punkte für die Heatmap gefunden.\")\n",
    "\n",
    "\n",
    "\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "print(result)\n",
    "map = create_heatmap(reduced_rdd)\n",
    "display(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==== **Untersuchung der Analysen** ===="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definition der Analyse- und Fehlertoleranz-Funktionen**\n",
    "\n",
    "- Systeminformationen:\n",
    "  Erfasst CPU- und RAM-Infos (z. B. Anzahl Kerne, Frequenz, Auslastung) und gibt sie formatiert aus.\n",
    "\n",
    "- Datenverteilung in RDDs:\n",
    "  Bestimmt die Anzahl der Datensätze pro Partition sowie die Gesamtzahl der Partitionen und Datensätze.\n",
    "\n",
    "- Fehlertoleranz:\n",
    "  - `cpu_stress`: Endlosschleife zum Auslasten einer CPU.  \n",
    "  - Netzwerkfehler simulieren und wiederherstellen (via `iptables`), indem Loopback-Verkehr blockiert bzw. entblockt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import psutil\n",
    "import datetime\n",
    "from multiprocessing import Process\n",
    "import os\n",
    "\n",
    "def get_system_info():\n",
    "    # CPU-Informationen\n",
    "    cpu_count = psutil.cpu_count(logical=False)  # Anzahl physikalischer CPU-Kerne\n",
    "    logical_cpu_count = psutil.cpu_count(logical=True)  # Anzahl logischer CPUs (mit Hyper-Threading)\n",
    "    cpu_freq = psutil.cpu_freq()  # CPU Frequenz\n",
    "    cpu_percent = psutil.cpu_percent(interval=1)  # CPU-Auslastung in Prozent\n",
    "\n",
    "    # RAM-Informationen\n",
    "    virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "    total_memory = virtual_memory.total  # Gesamtgröße des RAM\n",
    "    available_memory = virtual_memory.available  # Verfügbarer RAM\n",
    "    used_memory = virtual_memory.used  # Genutzter RAM\n",
    "    memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "\n",
    "\n",
    "    # Ausgabe der gesammelten Informationen\n",
    "    result = \"=\" * 50\n",
    "    result += \"System Informationsübersicht\"\n",
    "    result += \"=\" * 50\\\n",
    "\n",
    "    result += \"\\nCPU Infos:\\n\"\n",
    "    result += f\"  - Anzahl der physischen CPU-Kerne: {cpu_count}\\n\"\n",
    "    result += f\"  - Anzahl der logischen CPU-Kerne (inkl. Hyper-Threading): {logical_cpu_count}\\n\"\n",
    "    result += f\"  - Aktuelle CPU Frequenz: {cpu_freq.current} MHz\\n\"\n",
    "    result += f\"  - Aktuelle CPU Auslastung: {cpu_percent}%\\n\"\n",
    "\n",
    "    result += \"\\nRAM Infos:\\n\"\n",
    "    result += f\"  - Gesamter RAM: {total_memory / (1024 ** 3):.2f} GB\\n\"\n",
    "    result += f\"  - Verfügbarer RAM: {available_memory / (1024 ** 3):.2f} GB\\n\"\n",
    "    result += f\"  - Genutzter RAM: {used_memory / (1024 ** 3):.2f} GB\\n\"\n",
    "    result += f\"  - RAM Auslastung: {memory_percent}%\"\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "# Datenverteilung im CLuster\n",
    "def analyze_partition_distribution(rdd):\n",
    "    \"\"\"\n",
    "    Analysiert die Verteilung der Daten in einem RDD über die Partitionen.\n",
    "\n",
    "    :param rdd: Das RDD, das analysiert werden soll.\n",
    "    \"\"\"\n",
    "    # Funktion, um die Größe jeder Partition zu berechnen\n",
    "    def partition_sizes(index, iterator):\n",
    "        yield index, sum(1 for _ in iterator)\n",
    "\n",
    "   # Daten je Partition sammeln\n",
    "    partition_info = rdd.mapPartitionsWithIndex(partition_sizes).collect()\n",
    "\n",
    "    # Anzahl der Partitionen ermitteln\n",
    "    num_partitions = rdd.getNumPartitions()\n",
    "\n",
    "    #Ergebnisse formatieren\n",
    "    number_data = f\"Anzahl der Datensätze: {rdd.count()}\\n\"\n",
    "    number_partition = f\"Anzahl der Partitionen: {num_partitions}\\n\"\n",
    "    result = \"Datenverteilung auf Partitionen:\\n\"\n",
    "\n",
    "    # Ausgabe der Verteilung pro Partition\n",
    "    for partition, size in partition_info:\n",
    "        result += f\"Partition {partition}: {size} Datensätze\\n\"\n",
    "\n",
    "    return number_data, number_partition, result\n",
    "\n",
    "\n",
    "# Fehlertoleranz\n",
    "\n",
    "# Prozess, der die CPU lokal belastet\n",
    "def cpu_stress():\n",
    "    while True:\n",
    "        pass\n",
    "\n",
    "#if __name__ == \"__main__\":\n",
    "#    cpu_stress()\n",
    "\n",
    "\n",
    "# Simulieren von Netzwerkfehlern (Ausfall der Kommunikation im Netzwerk)\n",
    "def simulate_network_failure():\n",
    "    # Blockiere Loopback-Kommunikation (127.0.0.1), die Spark nutzt\n",
    "    os.system(\"iptables -A INPUT -s 127.0.0.1 -j DROP\")\n",
    "\n",
    "def restore_network():\n",
    "    # Entferne die Blockade\n",
    "    os.system(\"iptables -D INPUT -s 127.0.0.1 -j DROP\")\n",
    "\n",
    "\n",
    "# Knotenabschießen\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hardwareanalyse**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hardwareanalyse\n",
    "print(get_system_info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse 1 - Europäische Flughäfen**\n",
    "\n",
    "- Laufzeitmessung: Erfasst die Ausführungsdauer der Funktion `analyze_airports(...)` mithilfe von `time.time()`.  \n",
    "- Datenverteilung: Ruft `analyze_partition_distribution(airportRDD)` auf und zeigt anschließend die Gesamtanzahl der Datensätze, die Anzahl an Partitionen und die Verteilung der Datensätze über die Partitionen an.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Laufzeit\n",
    "start_time = time.time()\n",
    "analyze_airports(airport_data_path)\n",
    "end_time = time.time()\n",
    "print(f\"Die Funktion hat {(end_time - start_time):.4f} Sekunden benötigt\")\n",
    "\n",
    "#Datenverteilung\n",
    "number_data, number_partition, result = analyze_partition_distribution(airportRDD)\n",
    "print(number_data + number_partition + \"\\n\" + result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse 2 - Flugzeugbeschreibungen**\n",
    "\n",
    "- Laufzeitmessung: Stoppt die Zeit vor und nach dem Aufruf der Funktion `analyze_aircraft_data(...)`, um die Ausführungsdauer zu bestimmen.  \n",
    "- Datenverteilung: Ruft `analyze_partition_distribution(aircraftRDD)` auf und zeigt an, wie viele Datensätze es insgesamt gibt, wie viele Partitionen vorliegen und wie die Datensätze auf die Partitionen verteilt sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Laufzeit\n",
    "start_time = time.time()\n",
    "analyze_aircraft_data(aircraft_data_path)\n",
    "end_time = time.time()\n",
    "print(f\"Die Funktion hat {(end_time - start_time):.4f} Sekunden benötigt\")\n",
    "\n",
    "#Datenverteilung\n",
    "number_data, number_partition, result = analyze_partition_distribution(aircraftRDD)\n",
    "print(number_data + number_partition + \"\\n\" + result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse 3 - Bewegungsdaten**\n",
    "\n",
    "- Basiszeit: Führt die Funktion `analyze_flight_movements` zunächst mit einem sehr kleinen Prozentsatz (0.0001) der Daten aus und misst die dafür benötigte Zeit (Basismessung).  \n",
    "- Vollständige Daten: Ruft danach die gleiche Funktion mit 100 % der Daten auf und berechnet die Laufzeit abzüglich der zuvor ermittelten Basiszeit.  \n",
    "- Speicherauslastung: Liest nebenbei den aktuell verwendeten Arbeitsspeicher aus.  \n",
    "- Datenverteilung: Gibt zuletzt die Anzahl an Datensätzen und deren Verteilung auf die einzelnen Partitionen aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basiszeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=0.0001)\n",
    "virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "used_memory = virtual_memory.used  # Genutzter RAM\n",
    "memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "# Basiszeit errechnen\n",
    "base_time = end_time - start_time\n",
    "\n",
    "# Laufzeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktion aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=1.0)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Die Funktion hat für 100% der Daten {(end_time - start_time - base_time):.4f} Sekunden benötigt\")\n",
    "print(result_text)\n",
    "\n",
    "\n",
    "#Datenverteilung\n",
    "number_data, number_partition, result = analyze_partition_distribution(dataRDD)\n",
    "print(number_data + number_partition + result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Messungen**\n",
    "\n",
    "- Skalierbarkeitstest**: Untersucht, wie sich die Verarbeitung von Flugbewegungsdaten mit zunehmendem Datenvolumen verhält.\n",
    "- Datenmengen definieren**: Verarbeitet verschiedene Anteile der Daten (10%, 20%, 50%, 100%).\n",
    "- Leistungsmetriken erfassen**:\n",
    "  - Laufzeit: Misst die Ausführungszeit der Funktion `analyze_flight_movements` für jede Datenmenge.\n",
    "  - Datenverteilung: Analysiert die Verteilung der Daten über die Spark-Partitionen.\n",
    "  - Speicherauslastung: Erfasst die RAM-Auslastung nach der Verarbeitung.\n",
    "  - CPU-Auslastung: Misst die CPU-Auslastung unmittelbar nach der Verarbeitung.\n",
    "- Datenvisualisierung: Erstellt vier Diagramme zur Darstellung der gesammelten Leistungsmetriken in Abhängigkeit von der verarbeiteten Datenmenge:\n",
    "  1. Verarbeitungszeit vs. Datenmenge (%)\n",
    "  2. Anzahl der Partitionen** vs. Datenmenge (%)\n",
    "  3. RAM-Auslastung vs. Datenmenge (%)\n",
    "  4. CPU-Auslastung vs. Datenmenge (%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Skalierbarkeit\n",
    "\n",
    "# Definition der zu verarbeitenden Datenmenge\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "number_partition_data = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "\n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktion aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    \n",
    "    # Messwerte erheben\n",
    "    virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "    memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "    cpu_percent = psutil.cpu_percent(interval=None) # Messung direkt nach Ausführung der FUnktion\n",
    "\n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "        \n",
    "    # Daten für den Plot sammeln\n",
    "    number_data, number_partition, result = analyze_partition_distribution(dataRDD)\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time-start_time-base_time)  # Verarbeitungszeit abzüglich Basiszeit\n",
    "    number_partition_data.append(number_partition)\n",
    "    memory_data.append(memory_percent)\n",
    "    cpu_data.append(cpu_percent)\n",
    "\n",
    "# Plot 1: Laufzeit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, y_time, marker='o', linestyle='-', color='b')\n",
    "plt.title('Verarbeitungszeit in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('Laufzeit (Sekunden)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Anzahl der Partitionen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_data, number_partition_data, color='g', alpha=0.7)\n",
    "plt.title('Anzahl der Partitionen in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('Anzahl der Partitionen')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: RAM-Auslastung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, memory_data, marker='o', linestyle='-', color='r')\n",
    "plt.title('RAM-Auslastung in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('RAM-Auslastung (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: CPU-Auslastung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, cpu_data, marker='o', linestyle='-', color='y')\n",
    "plt.title('CPU-Auslastung in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('CPU-Auslastung (%)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse 4 - Kombinierte Flugobekte und Bewegungsdaten**\n",
    "\n",
    "- Basiszeitmessung:\n",
    "  - Führt die Funktionen `analyze_flight_movements`, `restructure_flight_movements`, `analyze_aircraft_data`, `combine_rdds` und `analyze_combined_rdd` mit einem sehr kleinen Datenanteil (`0.0001%`) aus.\n",
    "  - Misst die dafür benötigte Zeit, um eine Basiszeit (`base_time`) zu bestimmen.\n",
    "  \n",
    "- Laufzeitmessung:\n",
    "  - Führt dieselben Funktionen mit `100%` der Daten aus.\n",
    "  - Misst die gesamte Ausführungszeit und subtrahiert die zuvor ermittelte Basiszeit, um die tatsächliche Verarbeitungszeit zu berechnen.\n",
    "  \n",
    "- Datenverteilung:\n",
    "  - Analysiert die Verteilung der Daten im kombinierten RDD über die verschiedenen Partitionen mithilfe der Funktion `analyze_partition_distribution`.\n",
    "  \n",
    "- Ausgabe:\n",
    "  - Gibt die berechnete Laufzeit für die Verarbeitung der vollständigen Datenmenge sowie die Ergebnisse der Datenverteilungsanalyse aus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basiszeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time() \n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=0.0001)\n",
    "vehicle_rdd_rdd = restructure_flight_movements(dataRDD)\n",
    "result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd, 0.0001) #0.01% der Daten für Basiszeit\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "#Basiszeit errechnen\n",
    "base_time = end_time-start_time\n",
    "\n",
    "\n",
    "# Laufzeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "vehicle_rdd = restructure_flight_movements(dataRDD)    \n",
    "result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Die Funktion hat für 100% der Daten {end_time - start_time:.4f} Sekunden benötigt\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "\n",
    "# Datenverteilung\n",
    "number_data, number_partition, result = analyze_partition_distribution(combined_analyzed_rdd)\n",
    "print(number_data + number_partition + result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Messungen**\n",
    "\n",
    "- Skalierbarkeitstest:\n",
    "  - Definiert verschiedene Datenmengen (10%, 20%, 50%, 100%) zur Analyse der Verarbeitungseffizienz.\n",
    "  \n",
    "- Leistungsmetriken erfassen:\n",
    "  - Laufzeitmessung: Misst die Ausführungszeit der Funktion `analyze_flight_movements` für jede Datenmenge und subtrahiert die Basiszeit.\n",
    "  - Datenverteilung: Analysiert, wie die Daten über die Spark-Partitionen verteilt sind.\n",
    "  - Speicherauslastung: Erfasst die RAM-Auslastung nach der Verarbeitung.\n",
    "  - CPU-Auslastung: Misst die CPU-Auslastung unmittelbar nach der Verarbeitung.\n",
    "  \n",
    "- Datenvisualisierung:\n",
    "  - Erstellt vier Diagramme mit Matplotlib, um die gesammelten Leistungsmetriken in Abhängigkeit von der verarbeiteten Datenmenge darzustellen:\n",
    "    1. Verarbeitungszeit vs. **Datenmenge (%)\n",
    "    2. Anzahl der Partitionen vs. Datenmenge (%)\n",
    "    3. RAM-Auslastung vs. Datenmenge (%)\n",
    "    4. CPU-Auslastung vs. Datenmenge (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Skalierbarkeit\n",
    "\n",
    "# Definition der zu verarbeitenden Datenmenge\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "number_partition_data = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "    \n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktionen aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    vehicle_rdd_rdd = restructure_flight_movements(dataRDD)\n",
    "    result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "    combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "    combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd, percentage)\n",
    "    \n",
    "    # Messwerte erheben\n",
    "    virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "    memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "    cpu_percent = psutil.cpu_percent(interval=None) # Messung direkt nach Ausführung der FUnktion\n",
    "    \n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "    \n",
    "\n",
    "    # Daten für den Plot sammeln\n",
    "    number_data, number_partition, result = analyze_partition_distribution(combined_analyzed_rdd)\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time-start_time-base_time)  # Verarbeitungszeit abzüglich Basiszeit\n",
    "    number_partition_data.append(number_partition)\n",
    "    memory_data.append(memory_percent)\n",
    "    cpu_data.append(cpu_percent)\n",
    "  \n",
    "\n",
    "# Plot 1: Laufzeit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, y_time, marker='o', linestyle='-', color='b')\n",
    "plt.title('Verarbeitungszeit in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('Laufzeit (Sekunden)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Anzahl der Partitionen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_data, number_partition_data, color='g', alpha=0.7)\n",
    "plt.title('Anzahl der Partitionen in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('Anzahl der Partitionen')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: RAM-Auslastung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, memory_data, marker='o', linestyle='-', color='r')\n",
    "plt.title('RAM-Auslastung in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('RAM-Auslastung (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: CPU-Auslastung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, cpu_data, marker='o', linestyle='-', color='y')\n",
    "plt.title('CPU-Auslastung in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('CPU-Auslastung (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse 5 - Heatmap\n",
    "\n",
    "- Basiszeitmessung:\n",
    "  - Führt die Funktionen `analyze_flight_movements`, `process_and_reduce` und `create_heatmap` mit einem sehr kleinen Datenanteil (`0.0001%`) aus.\n",
    "  - Misst die dafür benötigte Zeit (`base_time`), um eine Basiszeit zu bestimmen.\n",
    "  \n",
    "- Laufzeitmessung:\n",
    "  - Führt dieselben Funktionen mit `100%` der Daten aus.\n",
    "  - Misst die gesamte Ausführungszeit und subtrahiert die zuvor ermittelte Basiszeit, um die tatsächliche Verarbeitungszeit zu berechnen.\n",
    "  \n",
    "- Datenverteilung: Analysiert die Verteilung der Daten im reduzierten RDD über die verschiedenen Partitionen mithilfe der Funktion `analyze_partition_distribution`.\n",
    "  \n",
    "- Ausgabe:\n",
    "  - Gibt die berechnete Laufzeit für die Verarbeitung der vollständigen Datenmenge sowie die Ergebnisse der Datenverteilungsanalyse aus.\n",
    "  - Erstellt und zeigt eine Heatmap basierend auf den reduzierten Koordinaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Basiszeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=0.0001)\n",
    "reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "map = create_heatmap(reduced_rdd)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "# Basiszeit errechenen\n",
    "base_time = end_time-start_time\n",
    "\n",
    "\n",
    "# Laufzeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "print(result)\n",
    "map = create_heatmap(reduced_rdd)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Die Funktion hat für 100% der Daten {(end_time - start_time - base_time):.4f} Sekunden benötigt\")\n",
    "print(result)\n",
    "print()\n",
    "\n",
    "\n",
    "# Datenverteilung\n",
    "number_data, number_partition, result = analyze_partition_distribution(reduced_rdd)\n",
    "print(number_data + number_partition + result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Messungen**\n",
    "\n",
    "- Basiszeitmessung:\n",
    "  - Führt die Funktionen `analyze_flight_movements`, `restructure_flight_movements`, `analyze_aircraft_data`, `combine_rdds` und `analyze_combined_rdd` mit einem sehr kleinen Datenanteil (`0.0001%`) aus.\n",
    "  - Misst die dafür benötigte Zeit (`base_time`), um eine Basiszeit zu bestimmen.\n",
    "\n",
    "- Laufzeitmessung:\n",
    "  - Führt dieselben Funktionen mit `100%` der Daten aus.\n",
    "  - Misst die gesamte Ausführungszeit und subtrahiert die zuvor ermittelte Basiszeit, um die tatsächliche Verarbeitungszeit zu berechnen.\n",
    "  - Gibt die berechnete Laufzeit sowie die Ergebnisse der Datenverteilungsanalyse aus.\n",
    "\n",
    "- Skalierbarkeitstest:\n",
    "  - Definiert verschiedene Datenmengen (`10%`, `20%`, `50%`, `100%`) zur Analyse der Verarbeitungseffizienz.\n",
    "  - Für jede Datenmenge:\n",
    "    - Führt die Funktionen `analyze_flight_movements`, `process_and_reduce` und `create_heatmap` aus.\n",
    "    - Erfasst Leistungsmetriken wie RAM-Auslastung und CPU-Auslastung.\n",
    "    - Analysiert die Datenverteilung über die Spark-Partitionen.\n",
    "    - Speichert die gesammelten Daten für die spätere Visualisierung.\n",
    "\n",
    "- Datenvisualisierung:\n",
    "  - Erstellt vier Diagramme mit Matplotlib, um die gesammelten Leistungsmetriken in Abhängigkeit von der verarbeiteten Datenmenge darzustellen:\n",
    "    1. Verarbeitungszeit vs. Datenmenge (%)\n",
    "    2. Anzahl der Partitionen vs. Datenmenge (%)\n",
    "    3. RAM-Auslastung vs. Datenmenge (%)\n",
    "    4. CPU-Auslastung vs. Datenmenge (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Skalierbarkeit\n",
    "\n",
    "# Definition der zu verarbeitenden Datenmenge\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "number_partition_data = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "    \n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktionen aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "    map = create_heatmap(reduced_rdd)\n",
    "   \n",
    "    # Messwerte erheben\n",
    "    virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "    memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "    cpu_percent = psutil.cpu_percent(interval=None) # Messung direkt nach Ausführung der FUnktion\n",
    "    \n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Daten für den Plot sammeln\n",
    "    number_data, number_partition, result = analyze_partition_distribution(combined_analyzed_rdd)\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time-start_time-base_time)  # Verarbeitungszeit abzüglich Basiszeit\n",
    "    number_partition_data.append(number_partition)\n",
    "    memory_data.append(memory_percent)\n",
    "    cpu_data.append(cpu_percent)\n",
    "\n",
    "# Plot 1: Laufzeit\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, y_time, marker='o', linestyle='-', color='b')\n",
    "plt.title('Verarbeitungszeit in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('Laufzeit (Sekunden)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Anzahl der Partitionen\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(x_data, number_partition_data, color='g', alpha=0.7)\n",
    "plt.title('Anzahl der Partitionen in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('Anzahl der Partitionen')\n",
    "plt.grid(axis='y')\n",
    "plt.show()\n",
    "\n",
    "# Plot 3: RAM-Auslastung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, memory_data, marker='o', linestyle='-', color='r')\n",
    "plt.title('RAM-Auslastung in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('RAM-Auslastung (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot 4: CPU-Auslastung\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, cpu_data, marker='o', linestyle='-', color='y')\n",
    "plt.title('CPU-Auslastung in Abhängigkeit von der Datenmenge')\n",
    "plt.xlabel('Datenmenge (%)')\n",
    "plt.ylabel('CPU-Auslastung (%)')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse 6: Fehlertoleranz und Laufzeiten bei Worker Node abschießen - Beispielhaft für die Funktion analyze_flight_movements**\n",
    "\n",
    "**Voraussetzungen:**\n",
    "1. Spark installiert\n",
    "2. Python installiert\n",
    "3. Java Version 11 installiert \n",
    "    - über brew: brew install openjdk@11\n",
    "4. Konfigurieren von spark config \n",
    "    - Pfad standardmäßig unter /usr/local/spark-<version>/conf\n",
    "    - ansonsten: find / -name \"spark-defaults.conf\" 2>/dev/null\n",
    "    - spark-env.sh.template kopieren und umbennen in spark-env.sh und folgende Parameter hinzufügen:\n",
    "        <br>SPARK_WORKER_INSTANCES=2\n",
    "        <br>SPARK_WORKER_CORES=4\n",
    "        <br>SPARK_WORKER_MEMORY=4g\n",
    "        <br>SPARK_MASTER_HOST=127.0.0.1\n",
    "    - (workers.template kopieren und umbennen in workers)\n",
    "5. Input-Dateien für Spark zugänglich machen (siehe Pfad im Coding)\n",
    "\n",
    "**Vorgehen:**\n",
    "1. Starten von Master und Worker Nodes unter localhost \n",
    "    - Spark Pfad öffnen: cd /usr/local/spark-3.5.3-bin-hadoop3\n",
    "    - Master: ./sbin/start-master.sh\n",
    "        -  Web-UI: http://localhost:8080\n",
    "    - Worker: ./sbin/start-worker.sh spark://localhost:7077\n",
    "2. Coding für analyze_flight_movements ausführen zur Definition der Funktion\n",
    "2. Starten des Codings für die Analyse der Fehlertoleranz. Dieses führt dann zunächst die Funktion regulär aus und misst die dafür benötigte Laufzeit.\n",
    "3. Coding fordert Enter-Eingabe auf. An diesem Punkt sind beliebig viele Worker Nodes zu killen, dann die Enter-Eingabe tätigen. \n",
    "    - Worker killen: kill -9 PID\n",
    "    - PID: kann unter dem Log Ordner in der jeweiligen Log-Datei für den Worker gefunden werden unter dem Abschnitt \"Started daemon with process name: (PID)\n",
    "    - Gekillte Worker sollten unter der Web-UI zu sehen sein\n",
    "4. Enter-Eingabe führt zur erneuten Ausführung der Funktion mit abgeschossenen Nodes.\n",
    "5. Worker/Master beenden:\n",
    "    - ./sbin/stop-all.sh\n",
    "\n",
    "**Ergänzend: **\n",
    "Zum Testen, wie die Ausführung des Codes währen der Laufzeit auf das Abschiessen von Worker Nodes reagiert, können diese auch während der ersten oder zweiten Ausführung gekillt werden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starte Analyse ohne Worker-Ausfall...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse ohne Worker-Ausfall abgeschlossen.\n",
      "Starte Analyse mit realem Worker-Ausfall...\n",
      "WICHTIG: Stoppen Sie einen Worker-Prozess während der Analyse manuell.\n",
      "Analyse mit realem Worker-Ausfall abgeschlossen.\n",
      "Ergebnisse für no_failure:\n",
      "Benötigte Zeit: 5.86 Sekunden\n",
      "Analyse-Ergebnisse: ('Die erfassten Bewegungen von Flug- und Bodenobjekten wurden im Zeitraum vom 2024/12/24 08:00:01 bis zum 2024/12/25 16:59:59 (Zeitzone: UTC) ermittelt.\\n==================================================\\nGeografische Abdeckung:\\nLängengrade: von -157.8554° bis 175.1297°\\nBreitengrade: von -46.1685° bis 68.8767°\\nDie Daten wurden in folgendem Luftraum erhoben.', <folium.folium.Map object at 0x124053890>, PythonRDD[18] at RDD at PythonRDD.scala:53)\n",
      "Ergebnisse für with_failure:\n",
      "Benötigte Zeit: 3.31 Sekunden\n",
      "Analyse-Ergebnisse: ('Die erfassten Bewegungen von Flug- und Bodenobjekten wurden im Zeitraum vom 2024/12/24 08:00:01 bis zum 2024/12/25 16:59:59 (Zeitzone: UTC) ermittelt.\\n==================================================\\nGeografische Abdeckung:\\nLängengrade: von -157.8554° bis 175.1297°\\nBreitengrade: von -46.1685° bis 68.8767°\\nDie Daten wurden in folgendem Luftraum erhoben.', <folium.folium.Map object at 0x1243e4550>, PythonRDD[19] at RDD at PythonRDD.scala:53)\n"
     ]
    }
   ],
   "source": [
    "# neue Spark Session unter localhost starten\n",
    "sc.stop()\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Spark-Konfigurationen festlegen\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set(\"spark.driver.bindAddress\", \"127.0.0.1\")  # Bindet den Driver an die lokale Adresse\n",
    "conf.set(\"spark.driver.host\", \"127.0.0.1\")        # Setzt die Host-Adresse für den Driver\n",
    "conf = conf.setMaster(\"spark://127.0.0.1:7077\")  # Verbindet sich mit dem Standalone-Master\n",
    "conf = conf.setAppName(\"Worker Ausfall Analyse\")  # Setzt den Namen der Spark-App\n",
    "\n",
    "# SparkContext erstellen\n",
    "try:\n",
    "    sc = pyspark.SparkContext(conf=conf)\n",
    "except Exception as e:\n",
    "    print(f\"Fehler:  {e}\")\n",
    "\n",
    "# Neue Dateipfade definieren, sodass sie für Spark zugänglich sind\n",
    "flight_data_path = '/usr/local/spark-3.5.3-bin-hadoop3/processed_data_reduced.csv'\n",
    "\n",
    "# Konfiguration für Spark-Session erstellen\n",
    "def create_spark_session(app_name=\"WorkerFailureTest\"):\n",
    "    conf = pyspark.SparkConf()\n",
    "    return SparkSession.builder.config(conf=conf).appName(app_name).getOrCreate()\n",
    "\n",
    "# Funktion zum Simulieren von Worker-Ausfällen\n",
    "def simulate_worker_failure(spark, analyze_function, flight_data_path, percentage):\n",
    "    \n",
    "    # Test ohne Worker-Ausfall\n",
    "    print(\"Starte Analyse ohne Worker-Ausfall...\")\n",
    "    start_time_no_failure = time.time()\n",
    "    result_no_failure = analyze_function(flight_data_path, percentage)\n",
    "    end_time_no_failure = time.time()\n",
    "    print(\"Analyse ohne Worker-Ausfall abgeschlossen.\")\n",
    "\n",
    "    # Ergebnisse protokollieren\n",
    "    result_log = {\n",
    "        \"no_failure\": {\n",
    "            \"time_taken\": end_time_no_failure - start_time_no_failure,\n",
    "            \"result_text\": result_no_failure,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Test mit realem Worker-Ausfall\n",
    "    print(\"Starte Analyse mit realem Worker-Ausfall...\")\n",
    "\n",
    "    print(\"WICHTIG: Stoppen Sie einen Worker-Prozess während der Analyse manuell.\")\n",
    "    input(\"Drücken Sie ENTER, sobald ein Worker-Node manuell gestoppt wurde...\")\n",
    "\n",
    "    try:\n",
    "        start_time_with_failure = time.time()\n",
    "        # Führe Analyse durch, während ein Worker fehlt\n",
    "        result_with_failure = analyze_function(flight_data_path, percentage)\n",
    "        end_time_with_failure = time.time()\n",
    "\n",
    "        print(\"Analyse mit realem Worker-Ausfall abgeschlossen.\")\n",
    "\n",
    "        result_log[\"with_failure\"] = {\n",
    "            \"time_taken\": end_time_with_failure - start_time_with_failure,\n",
    "            \"result_text\": result_with_failure,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        result_log[\"with_failure\"] = {\n",
    "            \"error\": str(e),\n",
    "        }\n",
    "\n",
    "    return result_log\n",
    "\n",
    "# Initialisierung der Testumgebung\n",
    "spark = create_spark_session()\n",
    "\n",
    "# Analyse durchführen und Ergebnisse protokollieren\n",
    "\n",
    "percentage = 0.1  # 10% der Daten nutzen\n",
    "result_log = simulate_worker_failure(spark, analyze_flight_movements, flight_data_path, percentage)\n",
    "\n",
    "# Protokoll ausgeben\n",
    "for scenario, details in result_log.items():\n",
    "    print(f\"Ergebnisse für {scenario}:\")\n",
    "    if \"error\" in details:\n",
    "        print(f\"Fehler: {details['error']}\")\n",
    "    else:\n",
    "        print(f\"Benötigte Zeit: {details['time_taken']:.2f} Sekunden\")\n",
    "        print(f\"Analyse-Ergebnisse: {details['result_text']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
