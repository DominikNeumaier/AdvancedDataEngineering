{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Data Projekt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation für das Projekt\n",
    "Flugvergeht ist ein spannendes Feld, weil man von \"unten\" gar nicht mitbekommt wie viele Flugzeuge tatsächlich über dem eigenen Kopf kreisen\n",
    "Janko ist selbst Pilot (Segelflugzeug SPL)\n",
    "Faszinierend wie Apps wie FlightRadar Geodaten von Tausenden Flugzeugen in Echtzeit analysieren / verknüfen und daraus einen Value schaffen\n",
    "Mit OpenSky Netwoork Zugang zu den welteiten Postitionsdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ziel des Projekts\n",
    "\n",
    "\n",
    "Bli bla blub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inhaltsverzeichnis\n",
    "1. [Cluster-Konfiguration](#1-cluster-konfiguration)\n",
    "    1. [Verfügbare Hardware-Ressourcen im Container](#11-verfügbare-hardware-ressourcen-im-container)\n",
    "    2. [Konfiguration und Starten des Spark Clusters](#12-konfiguration-und-starten-des-spark-cluster)\n",
    "    3. [Dateipfade festlegen und allgemeine Variablen](#13-dateipfade-festlegen-und-allgemeine-variable)\n",
    "    4. [Anlage des Spark Context](#14-anlage-des-spark-session)\n",
    "    5. [Anzeige der Spark WebUI](#15-anzeige-der-spark-webui)\n",
    "2. [Datenauswertung](#2-datenauswertungen)\n",
    "    1. [Auswertung - Weltweite Flughäfen](#21-auswertung---weltweite-flughäfen)\n",
    "    2. [Auswertung - Registrierte Flugobjekte](#22-auswertung---registrierte-flugobjekte)\n",
    "    3. [Auswertung - Bewegungsdaten von Flugobjekten](#23-auswertung---bewegungsdaten-von-flugobjekten)\n",
    "    4. [Auswertung - Kombination Flugobjekte und Bewegungsdaten](#24-auswertung---kombination-flugobjekte-und-bewegungsdaten)\n",
    "    5. [Auswertung - Flugobjekte und Verknüpfung mit Flughäfen](#25-auswertung---flugobjekte-und-verknüpfung-mit-flughäfen)\n",
    "    6. [Auswertung - Kombination Flugobjekte und Bewegungsdaten](#2)\n",
    "    7. [Auswertung - Visualisierung ausgewählter Bewegungsdaten](#27-auswertung---visualisierung-ausgewählter-bewegungsdaten)\n",
    "    8. [Auswertung - Visualisierung zufällige Bewegungsdaten](#28-auswertung---visualisierung-zufälliger-bewegungsdaten)\n",
    "    9. [Auswertung - Heatmap der  Bewegungsdaten](#29-auswertung---heatmap-der-bewegungsdaten)\n",
    "3. [Analyse der Auswertungen](#3-analyse-der-auswertungen)\n",
    "    1. [Definition der Analyse Funktionen](#31-definition-der-analyse-funktionen)\n",
    "    2. [Analyse - Weltweite Flughäfen](#32-analyse---auswertung---weltweite-flughäfen)\n",
    "    3. [Analyse - Registrierte Flugobjekte](#33-auswertung---registrierte-flugobjekte)\n",
    "    4. [Analyse - Bewegungsdaten von Flugobjekten](#34-analyse--bewegungsdaten-von-flugobjekten)\n",
    "    5. [Analyse - Bewegungsdaten von Flugobjekten](#35-analyse---kombination-flugobjekte-und-bewegungsdaten)\n",
    "    6. [Analyse - Heatmap der Bewegungsdaten](#36-analyse---heatmap-der-bewegungsdaten)\n",
    "    7. [Analyse - Skalierbarkeit bei veränderten Ressourcen](#37-analyse---skalierbarkeit-bei-veränderten-ressourcen)\n",
    "4. [Experimente](#4-weitere-experimente)\n",
    "    1. [Vorverarbeitung der Datenquelle](#41-vorverarbeitung-der-datenquelle)\n",
    "    2. [POC zu Parquet](#42-poc-zu-parquet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Cluster-Konfiguration\n",
    "### 1.1 Verfügbare Hardware-Ressourcen im Container\n",
    "\n",
    "Nachfolgend werden die auf Ihrer Maschine verfügbaren Hardware-Ressourcen angezeigt. \n",
    "Diese dienen als Anhaltspunkt für die Cluster-Konfiguration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Ihre verfügbaren Hardware-Ressourcen: ====\n",
      "RAM: 36.00 GB\n",
      "CPU Kerne: 12\n",
      "19.1\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "\n",
    "print(\"==== Ihre verfügbaren Hardware-Ressourcen: ====\")\n",
    "\n",
    "# RAM\n",
    "memory = psutil.virtual_memory()\n",
    "ram_gb = memory.total / (1024**3)\n",
    "print(f\"RAM: {ram_gb:.2f} GB\")\n",
    "\n",
    "# CPU Kerne\n",
    "cpu_count = psutil.cpu_count()\n",
    "print(f\"CPU Kerne: {cpu_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Konfiguration und Starten des Spark Cluster\n",
    "\n",
    "- Folgende Parameter wie gewünscht definieren:\n",
    "    - SPARK_WORKER_INSTANCES\n",
    "    - SPARK_WORKER_CORES\n",
    "    - SPARK_WORKER_MEMORY\n",
    "- Achte darauf die zuvor ermittelten Hardware-Ressourcen zu verwenden\n",
    "\n",
    "**!!! Achtung: Am Ende der Analysen werden die genutzten Ressourcen automatisch verdoppelt. Beachten bei der Konfiguration !!!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "SPARK_HOME = \"/usr/local/spark-3.5.0-bin-hadoop3/\"\n",
    "\n",
    "# Nutzerdefinierte Einstellungen\n",
    "WORKER_INSTANCES = 2  # Anzahl der Worker\n",
    "WORKER_CORES = 1      # Anzahl der CPU-Kerne pro Worker\n",
    "WORKER_MEMORY = \"2g\"  # Arbeitsspeicher pro Worker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starte Spark Cluster Setup ===\n",
      "Worker Count: 2\n",
      "Cores pro Worker: 1\n",
      "Memory pro Worker: 2g\n",
      "Beende vorhandene Spark-Prozesse...\n",
      "Starte Prozess mit Skript: /usr/local/spark-3.5.0-bin-hadoop3/sbin/start-master.sh\n",
      "Warte auf Master-Start...\n",
      "Starte 2 Worker...\n",
      "Starte Prozess mit Skript: /usr/local/spark-3.5.0-bin-hadoop3/sbin/start-worker.sh\n",
      "Starte Prozess mit Skript: /usr/local/spark-3.5.0-bin-hadoop3/sbin/start-worker.sh\n",
      "Aktive Worker PIDs: [1611]\n",
      "Cluster-Setup abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "import psutil\n",
    "import platform\n",
    "\n",
    "\n",
    "def find_spark_workers():\n",
    "    \"\"\"Findet alle laufenden Spark Worker Prozesse.\"\"\"\n",
    "    worker_pids = []\n",
    "    for proc in psutil.process_iter(['pid', 'cmdline']):\n",
    "        try:\n",
    "            if 'org.apache.spark.deploy.worker.Worker' in str(proc.info['cmdline']):\n",
    "                worker_pids.append(proc.pid)\n",
    "        except Exception:\n",
    "            continue\n",
    "    return worker_pids\n",
    "\n",
    "\n",
    "def kill_spark_processes():\n",
    "    \"\"\"Beendet alle laufenden Spark Master und Worker Prozesse.\"\"\"\n",
    "    print(\"Beende vorhandene Spark-Prozesse...\")\n",
    "    for proc in psutil.process_iter(['pid', 'cmdline']):\n",
    "        try:\n",
    "            cmdline = str(proc.info['cmdline'])\n",
    "            if 'org.apache.spark.deploy.master.Master' in cmdline or 'org.apache.spark.deploy.worker.Worker' in cmdline:\n",
    "                process = psutil.Process(proc.pid)\n",
    "                process.terminate()\n",
    "                time.sleep(1)\n",
    "                if process.is_running():\n",
    "                    process.kill()\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "\n",
    "def start_spark_process_unix(script_path, args):\n",
    "    \"\"\"Startet einen Spark-Prozess mit einer .sh-Datei auf macOS/Linux.\"\"\"\n",
    "    cmd = [script_path] + args\n",
    "    print(f\"Starte Prozess mit Skript: {script_path}\")\n",
    "    return subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "\n",
    "def configure_env_unix(worker_count, worker_cores, worker_memory):\n",
    "    # Pfad zur spark-env.sh\n",
    "    spark_env_path = os.path.join(SPARK_HOME, 'conf', 'spark-env.sh')\n",
    "    if not os.path.exists(spark_env_path):\n",
    "        raise FileNotFoundError(f\"Die Datei spark-env.sh wurde nicht gefunden unter: {spark_env_path}\")\n",
    "    \n",
    "    # spark-env.sh dynamisch anpassen\n",
    "    with open(spark_env_path, 'w') as spark_env:\n",
    "        spark_env.write(\"# Dynamisch generierte Spark-Umgebungskonfiguration\\n\")\n",
    "        spark_env.write(f\"SPARK_WORKER_CORES={worker_cores}\\n\")\n",
    "        spark_env.write(f\"SPARK_WORKER_INSTANCES={worker_count}\\n\")\n",
    "        spark_env.write(f\"SPARK_WORKER_MEMORY={worker_memory}\\n\")\n",
    "        spark_env.write(\"SPARK_MASTER_HOST=127.0.0.1\\n\")\n",
    "\n",
    "        # Sicherstellen, dass die Datei geschrieben und geschlossen wird\n",
    "        spark_env.flush()\n",
    "        os.fsync(spark_env.fileno())  # Sicherstellen, dass die Änderungen auf die Festplatte geschrieben werden\n",
    "\n",
    "\n",
    "def setup_spark_cluster(worker_count, worker_cores, worker_memory):\n",
    "    \"\"\"Startet einen Spark Cluster mit Master und Workers.\"\"\"\n",
    "    print(\"\\n=== Starte Spark Cluster Setup ===\")\n",
    "    print(f\"Worker Count: {worker_count}\")\n",
    "    print(f\"Cores pro Worker: {worker_cores}\")\n",
    "    print(f\"Memory pro Worker: {worker_memory}\")\n",
    "\n",
    "    # Alte Prozesse beenden\n",
    "    kill_spark_processes()\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "    #Umgebungsvariablen anpassen\n",
    "    configure_env_unix(worker_count, worker_cores, worker_memory)\n",
    "\n",
    "    master_script = os.path.join(SPARK_HOME, 'sbin', 'start-master.sh')\n",
    "    master_process = start_spark_process_unix(master_script, [])\n",
    "    \n",
    "    print(\"Warte auf Master-Start...\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Workers starten\n",
    "    worker_processes = []\n",
    "    print(f\"Starte {worker_count} Worker...\")\n",
    "    for i in range(worker_count):\n",
    "\n",
    "        worker_script = os.path.join(SPARK_HOME, 'sbin', 'start-worker.sh')\n",
    "        worker = start_spark_process_unix(worker_script, ['spark://localhost:7077'])\n",
    "        \n",
    "        worker_processes.append(worker)\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Status prüfen\n",
    "    time.sleep(3)\n",
    "    worker_pids = find_spark_workers()\n",
    "    print(f\"Aktive Worker PIDs: {worker_pids}\")\n",
    "    print(\"Cluster-Setup abgeschlossen!\")\n",
    "    \n",
    "    return worker_pids\n",
    "\n",
    "\n",
    "def cleanup_spark_cluster():\n",
    "    \"\"\"Beendet den Spark Cluster.\"\"\"\n",
    "    print(\"\\n=== Beende Spark Cluster ===\")\n",
    "    kill_spark_processes()\n",
    "    print(\"Alle Spark-Prozesse beendet.\")\n",
    "\n",
    "\n",
    "# Cluster starten\n",
    "worker_pids = setup_spark_cluster(\n",
    "    worker_count=WORKER_INSTANCES,\n",
    "    worker_cores=WORKER_CORES,\n",
    "    worker_memory=WORKER_MEMORY\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dateipfade festlegen und allgemeine Variable\n",
    "\n",
    "- `airport_data_path`: Pfad zur CSV-Datei mit Flughafendaten (`airports.csv`).\n",
    "- `aircraft_data_path`: Pfad zur CSV-Datei mit vollständigen Flugzeugdaten (`aircraft-database-complete-2024-10.csv`).\n",
    "- `flight_data_path`: Pfad zur CSV-Datei mit vorverarbeiteten Flugbewegungsdaten (`processed_data_reduced.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_data_path = \"data/airports.csv\"\n",
    "aircraft_data_path = 'data/aircraft-database-complete-2024-10.csv'\n",
    "flight_data_path = 'data/processed_data_reduced.csv'\n",
    "master_url = \"http://localhost:4040\"  \n",
    "dquote='\\\"'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Anlage des Spark Session\n",
    "Spark-Context erstellen und notwendige Frameworks importieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WORKER_MEMORY' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mconfig(conf\u001b[38;5;241m=\u001b[39mconf)\u001b[38;5;241m.\u001b[39mappName(app_name)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Spark-Session erstellen\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m worker_memory \u001b[38;5;241m=\u001b[39m \u001b[43mWORKER_MEMORY\u001b[49m\n\u001b[1;32m     19\u001b[0m spark \u001b[38;5;241m=\u001b[39m create_spark_session(worker_memory)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Versuch, den SparkContext zu erstellen\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WORKER_MEMORY' is not defined"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Funktion zur Erstellung einer Spark-Session mit Konfiguration\n",
    "def create_spark_session(worker_memory, app_name=\"AdvancedDataProject\"):\n",
    "    # Spark-Konfigurationen festlegen\n",
    "    conf = pyspark.SparkConf()\n",
    "    conf.set(\"spark.driver.bindAddress\", \"127.0.0.1\")   # Bindet den Driver an die lokale Adresse\n",
    "    conf.set(\"spark.driver.host\", \"127.0.0.1\")          # Setzt die Host-Adresse für den Driver\n",
    "    conf.setMaster(\"spark://127.0.0.1:7077\")            # Verbindet sich mit dem Standalone-Master\n",
    "    conf.set(\"spark.executor.memory\", worker_memory)\n",
    "    conf.setAppName(app_name)                           # Setzt den Namen der Spark-App\n",
    "\n",
    "    # SparkSession erstellen und zurückgeben\n",
    "    return SparkSession.builder.config(conf=conf).appName(app_name).getOrCreate()\n",
    "\n",
    "# Spark-Session erstellen\n",
    "worker_memory = WORKER_MEMORY\n",
    "spark = create_spark_session(worker_memory)\n",
    "\n",
    "# Versuch, den SparkContext zu erstellen\n",
    "try:\n",
    "   sc = spark.sparkContext  # Verwende den SparkContext aus der SparkSession\n",
    "   print(\"SparkContext erfolgreich erstellt.\")\n",
    "   print(f\"Spark läuft auf Master: {sc.master}\")\n",
    "except Exception as e:\n",
    "   print(f\"Fehler beim Erstellen des SparkContext: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Anzeige der Spark WebUI <a id=\"webui\"></a>\n",
    "\n",
    "Unter folgendem Link ist nun die Konfiguration des Clusters einzusehen:\n",
    "http://localhost:8080/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Datenauswertungen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Auswertung - Weltweite Flughäfen <a id=\"flughäfen\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Liest eine CSV-Datei mit weltweiten Flughafendaten ein und verarbeitet sie als RDD.  \n",
    "- Filtert nach ausgewählten Spalten, entfernt den Header und parst die Daten korrekt als CSV.  \n",
    "- Bestimmt die Anzahl aller relevanten Flughäfen und zählt diese nach Typ.  \n",
    "- Gibt einen formatierten Ergebnis-String und das RDD zurück.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem -> Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten -> Einfach zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Zählungen im Speicher -> Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimenisonierung eines realen Systems\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from io import StringIO\n",
    "\n",
    "def analyze_airports(airport_data_path):\n",
    "    \"\"\"\n",
    "    Analysiert weltweite Flughäfen und Heliports basierend auf einer CSV-Datei und gibt \n",
    "    eine formatierte Übersicht sowie die gefilterten Daten als RDD zurück.\n",
    "\n",
    "    Die Funktion führt folgende Schritte aus:\n",
    "    1. Liest die CSV-Datei ein und entfernt den Header.\n",
    "    2. Parsiert die Daten korrekt, um mögliche Fehler durch Anführungszeichen zu vermeiden.\n",
    "    3. Filtert die Daten nach bestimmten Typen von Flughäfen\n",
    "       (z. B. \"small_airport\", \"medium_airport\", \"large_airport\").\n",
    "    4. Gibt die Gesamtanzahl der Flughäfen und die Anzahl pro Kategorie aus.\n",
    "\n",
    "    :param airport_data_path: Der Dateipfad zur CSV-Datei mit den Flughafendaten.\n",
    "                              Erwartet eine Datei mit mindestens 8 Spalten, \n",
    "                              wobei relevante Spalten die Spalten 1, 2, 3, 4, 5 sind.\n",
    "    :return: Ein Tuple mit zwei Elementen:\n",
    "             - Ein formatierter String, der die Analyse zusammenfasst.\n",
    "             - Ein RDD mit den gefilterten Flughafendaten, bestehend aus:\n",
    "               (Name, Typ, Breitengrad, Längengrad, Höhe).\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten einlesen und Header extrahieren\n",
    "    airportRDD = sc.textFile(airport_data_path)\n",
    "    header = airportRDD.first()\n",
    "\n",
    "    # CSV korrekt parsen, um Fehler durch Kommata innerhalb von Anführungszeichen zu vermeiden\n",
    "    def parse_csv(line):\n",
    "        reader = csv.reader(StringIO(line))\n",
    "        return next(reader)\n",
    "\n",
    "    # Filterkriterien anwenden und nur die relevanten Spalten speichern (1, 2, 3, 4, 5, 7)\n",
    "    airportRDD = (\n",
    "        airportRDD.filter(lambda line: line != header)  # Header entfernen\n",
    "                  .map(parse_csv)  # CSV korrekt parsen\n",
    "                  .filter(lambda cols: len(cols) > 7)  # Sicherstellen, dass genug Spalten vorhanden sind\n",
    "                  .map(lambda cols: (cols[1].strip('\"'),  # Name\n",
    "                                    cols[2].strip('\"'),  # Typ\n",
    "                                    cols[3].strip('\"'),  # Breitengrad\n",
    "                                    cols[4].strip('\"'),  # Längengrad\n",
    "                                    cols[5].strip('\"'),  # Höhe\n",
    "                                    ))\n",
    "                  .filter(lambda cols: cols[1] in [\"small_airport\", \"medium_airport\", \"large_airport\"])  # Nur bestimmte Typen\n",
    "    )\n",
    "\n",
    "    # Anzahl der Flughäfen insgesamt\n",
    "    total_airports = airportRDD.count()\n",
    "\n",
    "    # Anzahl der Flughäfen nach Kategorien\n",
    "    categorized_airports = (\n",
    "        airportRDD.map(lambda cols: cols[1])  # Typ (Spalte 2)\n",
    "                  .countByValue()\n",
    "    )\n",
    "\n",
    "    # Formatierte Ausgabe als String erstellen\n",
    "    result = \"Analyse weltweiter Flughäfen\\n\"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "    result += f\"Die Anzahl der weltweiten Flughäfen beträgt: {total_airports}\\n\"\n",
    "    result += f\"Davon kleine Flughäfen: {categorized_airports.get('small_airport', 0)}\\n\"\n",
    "    result += f\"Davon mittelgroße Flughäfen: {categorized_airports.get('medium_airport', 0)}\\n\"\n",
    "    result += f\"Davon große Flughäfen: {categorized_airports.get('large_airport', 0)}\\n\"\n",
    "\n",
    "    return result, airportRDD\n",
    "\n",
    "# Beispielaufruf und Ausgabe\n",
    "result, airportRDD = analyze_airports(airport_data_path)\n",
    "print(result)\n",
    "\n",
    "# Optional: Die Daten aus airportRDD anzeigen lassen\n",
    "#airportRDD.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Auswertung - Registrierte Flugobjekte\n",
    "\n",
    "Liest eine CSV-Datei mit Flugzeugdaten und entfernt den Header.\n",
    "- Zählt die Anzahl registrierter Flugobjekte.\n",
    "- Ermittelt die Top 20 Airlines nach Anzahl der registrierten Flugzeuge.\n",
    "- Gibt einen formatierten String und zwei RDDs zurück (gefilterte Daten und Airlines mit Anzahl).\n",
    "\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Einfach zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_aircraft_data(aircraft_data_path):\n",
    "    \n",
    "    \"\"\"\n",
    "    Analysiert Flugzeugdaten aus einer CSV-Datei und liefert eine Übersicht über die \n",
    "    registrierten Flugobjekte sowie die Top 20 Airlines nach Anzahl registrierter Flugzeuge.\n",
    "\n",
    "    Die Funktion führt folgende Schritte aus:\n",
    "    1. Liest die CSV-Daten ein und entfernt die Kopfzeile.\n",
    "    2. Berechnet die Gesamtanzahl der registrierten Flugobjekte.\n",
    "    3. Aggregiert die Anzahl der Flugzeuge pro Airline und sortiert sie nach Häufigkeit.\n",
    "    4. Gibt die Top 20 Airlines mit den meisten registrierten Flugzeugen aus.\n",
    "\n",
    "    :param aircraft_data_path: Der Dateipfad zur CSV-Datei mit Flugzeugregistrierungsdaten. \n",
    "                               Die Datei sollte mindestens 18 Spalten enthalten, wobei \n",
    "                               die Airline-Information in Spalte 17 erwartet wird.\n",
    "    \n",
    "    :return: Ein Tupel mit drei Elementen:\n",
    "        - result (str): Ein formatierter String mit der Gesamtanzahl der Flugobjekte \n",
    "                        und den Top 20 Airlines nach Anzahl registrierter Flugzeuge.\n",
    "        - aircraft_raw_rdd (RDD): Ein Spark RDD mit den Rohdaten (ohne Kopfzeile).\n",
    "        - aircraftRDD (RDD): Ein Spark RDD mit Airline-Namen und der Anzahl ihrer Flugzeuge \n",
    "                             (als Key-Value-Paare).\n",
    "    \"\"\"\n",
    "\n",
    "    # Daten einlesen und Header extrahieren\n",
    "    aircraft_input_rdd = sc.textFile(aircraft_data_path)\n",
    "    header = aircraft_input_rdd.first()\n",
    "\n",
    "    # Filterkriterien anwenden: Entfernen des Headers\n",
    "    aircraft_raw_rdd = aircraft_input_rdd.filter(lambda line: line != header)\n",
    "    \n",
    "    # Anzahl der registrierten Flugobjekte\n",
    "    Anzahl_registrierte_Flugobjekte = aircraft_raw_rdd.count()\n",
    "\n",
    "    # RDD erstellen, um die Airlines nach Anzahl der registrierten Flugzeuge zu zählen\n",
    "    aircraftRDD = (\n",
    "        aircraft_raw_rdd.filter(lambda line: len(line.split(\",\")) > 16)\n",
    "        .map(lambda line: line.split(\",\")[17].strip('\"'))\n",
    "        .filter(lambda airline: airline != \"''\")\n",
    "        .map(lambda airline: (airline, 1))\n",
    "        .reduceByKey(lambda a, b: a + b)\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "    )\n",
    "    \n",
    "    # Top 20 Airlines nach Anzahl der registrierten Flugzeuge\n",
    "    top_20_airlines = aircraftRDD.take(20)\n",
    "\n",
    "    # Ergebnis als formatierten String zurückgeben\n",
    "    result = f\"Anzahl der weltweit gemeldeten, eindeutigen Flug- und Bodenobjekte: {Anzahl_registrierte_Flugobjekte}\\n\"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "    result += \"Top 20 Airlines nach Anzahl der registrierten Flugzeuge:\\n\"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "    for airline, count in top_20_airlines:\n",
    "        result += f\"{airline}: {count}\\n\"\n",
    "\n",
    "    return result, aircraft_raw_rdd, aircraftRDD\n",
    "\n",
    "# Beispielaufruf und Ausgabe\n",
    "result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Auswertung - Bewegungsdaten von Flugobjekten\n",
    "\n",
    "- Liest Flugbewegungsdaten ein, entfernt Header und ungültige Einträge.\n",
    "- Nutzt einen einstellbaren Prozentsatz der Daten (`sampleRDD`).\n",
    "- Bestimmt den zeitlichen Rahmen (erstes und letztes Datum) sowie min./max. Längen- und Breitengrad.\n",
    "- Berechnet das Zentrum und erzeugt eine Folium-Karte mit einem Rechteck des betrachteten Luftraums.\n",
    "- Gibt einen Ergebnis-Text, die Karte und das gefilterte RDD zurück.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "    - Skaliarbeitkeit bei verschiedenen Datenmengen analysiert (siehe ==== Untersuchung der Analysen ====)\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Folium: Erstellen und Rendern der Karte erfolgt auf dem Driver, bei großen geografischen Bereichen viel Speicher und CPU\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Teilweise Ausfälle beeinflussen nicht den Prozess\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung der Visualisierung bspw. durch separate Ressourcen für die Generierung der Folium-Karte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "def analyze_flight_movements(flight_data_path, percentage=1.0):\n",
    "    \"\"\"\n",
    "    Analysiert Bewegungsdaten von Flugzeugen und erstellt eine geografische Visualisierung des analysierten Bereichs.\n",
    "    Misst die Laufzeit für die Verarbeitung eines benutzerdefinierten Prozentsatzes der Daten.\n",
    "\n",
    "    Die Funktion führt folgende Schritte aus:\n",
    "    1. Liest die Bewegungsdaten aus einer CSV-Datei ein und entfernt ungültige Einträge.\n",
    "    2. Reduziert die Daten auf einen angegebenen Prozentsatz.\n",
    "    3. Bestimmt den geografischen Bereich der Bewegungsdaten (Min/Max-Längen- und Breitengrade).\n",
    "    4. Berechnet den erfassten Zeitraum der Daten.\n",
    "    5. Erstellt eine Karte, die den geografischen Bereich als Rechteck anzeigt.\n",
    "\n",
    "    :param flight_data_path: Der Dateipfad zur CSV-Datei mit den Bewegungsdaten.\n",
    "    :param percentage: Prozentsatz (zwischen 0 und 1), der die Datenmenge angibt, \n",
    "                       die für die Analyse verwendet werden soll (Standard: 1.0 für 100%).\n",
    "    :raises ValueError: Wenn der Prozentsatz nicht zwischen 0 und 1 liegt.\n",
    "    \n",
    "    :return: Ein Tuple mit folgenden Elementen:\n",
    "        - result_text (str): Ein Text mit den Analyseergebnissen, einschließlich \n",
    "                             des erfassten Zeitraums und des geografischen Bereichs.\n",
    "        - map (folium.Map): Eine Karte mit dem geografischen Bereich der analysierten Daten.\n",
    "        - dataRDD (RDD): Ein Spark RDD mit den gefilterten und zufällig ausgewählten Daten.\n",
    "    \"\"\"\n",
    "\n",
    "    if not 0 < percentage <= 1:\n",
    "        raise ValueError(\"Percentage muss zwischen 0 und 1 liegen\")\n",
    "    \n",
    "    # Einlesen der Bewegungsdaten eines definierten Zeitraums in einem bestimmten geografischen Bereich\n",
    "    myFileRDD = sc.textFile(flight_data_path)\n",
    "    header = myFileRDD.take(1)\n",
    "    sampleRDD_with_null = myFileRDD.filter(lambda line: line not in header)\n",
    "    sampleRDD = sampleRDD_with_null.filter(lambda line: \"NULL\" not in line)\n",
    "\n",
    "    #Reduktion des RDDs auf Prozentwert an Datensätzen, seed sorgt für reproduzierbare Zufallsergebnisse (42 ist die Antwort auf alles).\n",
    "    dataRDD = sampleRDD.sample(False, percentage, seed=42)\n",
    "\n",
    "    #Erfassen des Zeitraums in dem Daten die erhoben wurden\n",
    "    start_time = dataRDD.map(lambda line: line.split(',')[0]).min()\n",
    "    end_time = dataRDD.map(lambda line: line.split(',')[0]).max()\n",
    "\n",
    "    #Geographische Analyse des betrachteten Luftraums\n",
    "\n",
    "    # Längengrade sortieren und auslesen\n",
    "    min_longitude = dataRDD.map(lambda line: float(line.split(',')[3])).min()\n",
    "    max_longitude = dataRDD.map(lambda line: float(line.split(',')[3])).max()\n",
    "\n",
    "    # Breitengrade (vermutlich Spalte mit Index 5 und 6)\n",
    "    min_latitude = dataRDD.map(lambda line: float(line.split(',')[2])).min()\n",
    "    max_latitude = dataRDD.map(lambda line: float(line.split(',')[2])).max()\n",
    "\n",
    "    # Zentrum des Bereichs berechnen\n",
    "    center_lat = (min_latitude + max_latitude) / 2\n",
    "    center_lon = (min_longitude + max_longitude) / 2\n",
    "    result_text = (\n",
    "        f\"Die erfassten Bewegungen von Flug- und Bodenobjekten wurden im Zeitraum vom \"\n",
    "        f\"{start_time.replace(dquote, '').replace('+00', '')} bis zum \"\n",
    "        f\"{end_time.replace(dquote, '').replace('+00', '')} (Zeitzone: UTC) ermittelt.\\n\"\n",
    "        f\"{'=' * 50}\\n\"\n",
    "        \"Geografische Abdeckung:\\n\"\n",
    "        f\"Längengrade: von {min_longitude:.4f}° bis {max_longitude:.4f}°\\n\"\n",
    "        f\"Breitengrade: von {min_latitude:.4f}° bis {max_latitude:.4f}°\\n\"\n",
    "        \"Die Daten wurden in folgendem Luftraum erhoben.\"\n",
    "    )\n",
    "\n",
    "    # Karte erstellen\n",
    "    map = folium.Map(location=[center_lat, center_lon], zoom_start=5)\n",
    "\n",
    "    # Rechteck auf der Karte hinzufügen\n",
    "    bounds = [[min_latitude, min_longitude], [max_latitude, max_longitude]]\n",
    "    folium.Rectangle(\n",
    "        bounds=bounds,\n",
    "        color=\"red\",\n",
    "        weight=2,\n",
    "        fill=True,\n",
    "        fill_color=\"blue\",\n",
    "        fill_opacity=0.2,\n",
    "        popup=\"Geographischer Bereich\"\n",
    "    ).add_to(map)\n",
    "\n",
    "    return result_text, map, dataRDD  \n",
    "\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "print(result_text)\n",
    "map_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Auswertung - Kombination Flugobjekte und Bewegungsdaten\n",
    "\n",
    "1. Gruppierung der Bewegungsdaten nach Flugzeug-ID\n",
    "2. Bereinigung der Registrierungsdaten\n",
    "3. Kombination von Bewegungsdaten und Registrierungsdaten\n",
    "4. Analyse des kombinerten RDDs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gruppierung der Bewegungsdaten nach Flugzeug-ID\n",
    "\n",
    "- Parst das `dataRDD`, um die Bewegungsdaten nach Flugzeug-ID (icao24) zu gruppieren.  \n",
    "- Jede Flugzeug-ID wird einem Dictionary zugeordnet, das relevante Bewegungsdaten wie Zeit, Breitengrad, Längengrad und On-Ground-Status enthält.  \n",
    "- Gibt ein RDD zurück, das nach Flugzeug-ID gruppiert ist, wobei jeder Schlüssel (Flugzeug-ID) eine Liste von Bewegungsdaten-Dictionaries als Wert enthält."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def restructure_flight_movements(dataRDD):\n",
    "    \"\"\"\n",
    "    Analysiert Roh-Bewegungsdaten und gruppiert sie nach Flugzeug-ID.\n",
    "\n",
    "    Diese Funktion transformiert die Roh-Bewegungsdaten und gruppiert sie basierend auf der Flugzeug-ID (icao24). \n",
    "    Jede Flugzeug-ID wird einem Dictionary zugeordnet, das die relevanten Bewegungsdaten (Zeit, Breitengrad, Längengrad und On-Ground-Status) enthält.\n",
    "\n",
    "    :param dataRDD: Ein RDD mit Roh-Bewegungsdaten. Erwartet werden die folgenden Spalten:\n",
    "                    - Spalte 0: Zeitstempel\n",
    "                    - Spalte 1: Flugzeug-ID (icao24)\n",
    "                    - Spalte 2: Breitengrad\n",
    "                    - Spalte 3: Längengrad\n",
    "                    - Spalte 4: On-Ground-Status (1 für am Boden, 0 für in der Luft)\n",
    "                    \n",
    "    :return: Ein RDD, das nach Flugzeug-ID gruppiert ist. Jeder Schlüssel (Flugzeug-ID) hat eine Liste von Dictionaries als Wert, \n",
    "             die die Bewegungsdaten (Zeit, Breitengrad, Längengrad, On-Ground-Status) enthalten.\n",
    "    \"\"\"\n",
    "    \n",
    "    # RDD transformieren: Daten in ein Tupel (Flugzeug-ID, Bewegungsdaten) umwandeln\n",
    "    vehicle_rdd = dataRDD.map(lambda line: (\n",
    "        line.split(',')[1].strip(' \"\\t\\r\\n'),  # Flugzeug-ID (icao24)\n",
    "        {  # Bewegungsdaten als Dictionary\n",
    "            \"time\": line.split(',')[0].strip('\"'),\n",
    "            \"lat\": float(line.split(',')[2].strip('\"')),\n",
    "            \"lon\": float(line.split(',')[3].strip('\"')),\n",
    "            \"onground\": int(line.split(',')[4].strip('\"'))\n",
    "        }\n",
    "    ))\n",
    "    \n",
    "    # Gruppiere nach Flugzeug-ID und wandle die Werte in Listen um\n",
    "    vehicle_rdd = vehicle_rdd.groupByKey().mapValues(list)\n",
    "    \n",
    "    return vehicle_rdd\n",
    "\n",
    "# Beispielaufruf der Funktion\n",
    "vehicle_rdd = restructure_flight_movements(dataRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bereinigung der Registrierungsdaten und Extraktion relevanter Informationen\n",
    "\n",
    "- Verarbeitet das `aircraft_raw_rdd`, um die relevanten Informationen zu registrierten Flugzeugen zu extrahieren.  \n",
    "- Extrahiert für jedes Flugzeug die Flugzeug-ID (icao24) sowie Details wie Beschreibung, Hersteller, Modell, Typ und Airline.  \n",
    "- Gibt ein bereinigtes RDD zurück, in dem die Flugzeug-ID als Schlüssel und ein Dictionary mit den extrahierten Registrierungsinformationen als Wert gespeichert ist.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_aircraft_data(aircraft_raw_rdd):\n",
    "    \"\"\"\n",
    "    Bereinigt die Rohdaten zu registrierten Flug- und Bodenobjekten und extrahiert relevante Informationen.\n",
    "\n",
    "    Die Funktion verarbeitet die Rohdaten und erstellt ein RDD mit der Flugzeug-ID (icao24) \n",
    "    als Schlüssel und einem Dictionary mit den folgenden Informationen als Wert:\n",
    "        - Beschreibung\n",
    "        - Hersteller\n",
    "        - Modell\n",
    "        - Typ\n",
    "        - Airline\n",
    "\n",
    "    :param aircraft_raw_rdd: Ein Spark RDD mit Rohdaten zu registrierten Flug- und Bodenobjekten. \n",
    "                             Erwartet CSV-Daten mit der Flugzeug-ID (icao24) in Spalte 0 und weiteren \n",
    "                             Details in späteren Spalten.\n",
    "\n",
    "    :return: Ein bereinigtes Spark RDD, bei dem der Schlüssel die Flugzeug-ID (icao24) und der Wert \n",
    "             ein Dictionary mit den extrahierten Registrierungsinformationen ist.\n",
    "    \"\"\"\n",
    "    def parse_type(line):\n",
    "        parts = line.split(',')\n",
    "        return (\n",
    "            parts[0].strip(\"'\"),  # Flugzeug-ID (icao24)\n",
    "            {\n",
    "                \"description\": parts[5].strip(\"'\") if len(parts) > 4 else \"\",\n",
    "                \"manufacturer\": parts[13].strip(\"'\") if len(parts) > 12 else \"\",\n",
    "                \"model\": parts[14].strip(\"'\") if len(parts) > 14 else \"\",\n",
    "                \"type\": parts[15].strip(\"'\") if len(parts) > 15 else \"\",\n",
    "                \"airline\": parts[18].strip(\"'\") if len(parts) > 16 else \"\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # RDD bereinigen und nur relevante Daten aufnehmen\n",
    "    cleaned_aircraft_rdd = aircraft_raw_rdd.map(parse_type)\n",
    "    return cleaned_aircraft_rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kombination der Bewegungsdaten mit den bereinigten Registrierungsdaten\n",
    "\n",
    "- Kombiniert das `vehicle_rdd`, das die Bewegungsdaten enthält, mit dem `cleaned_aircraft_rdd`, das die bereinigten Registrierungsdaten der Flugzeuge enthält.  \n",
    "- Die beiden RDDs werden anhand der Flugzeug-ID (icao24) über ein `LeftOuterJoin()` miteinander verknüpft, um vollständige Informationen zu jedem Flugzeug zu erzeugen.  \n",
    "- Gibt ein RDD zurück, das für jede Flugzeug-ID sowohl die Bewegungsdaten als auch die Registrierungsinformationen enthält, wobei die Flugdaten nach Zeit sortiert sind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_rdds(vehicle_rdd, cleaned_aircraft_rdd):\n",
    "    \"\"\"\n",
    "    Kombiniert die Bewegungsdaten (vehicle_rdd) mit bereinigten Registrierungsdaten (cleaned_aircraft_rdd), \n",
    "    um vollständige Informationen zu jedem Fahrzeug zu erzeugen.\n",
    "\n",
    "    Die Funktion kombiniert die beiden RDDs anhand der Flugzeug-ID (icao24) \n",
    "    und erstellt ein RDD, das Flugdaten und Registrierungsinformationen für jedes Fahrzeug enthält.\n",
    "\n",
    "    :param vehicle_rdd: Ein Spark RDD mit Bewegungsdaten. \n",
    "                        Erwartet Key-Value-Paare, bei denen der Schlüssel die Flugzeug-ID (icao24) ist \n",
    "                        und der Wert eine Liste von Flugdaten enthält.\n",
    "    :param cleaned_aircraft_rdd: Ein Spark RDD mit bereinigten Registrierungsdaten, \n",
    "                                 das die Flugzeug-ID (icao24) als Schlüssel und ein Dictionary \n",
    "                                 mit Registrierungsinformationen als Wert enthält.\n",
    "\n",
    "    :return: Ein Spark RDD mit folgenden Elementen:\n",
    "        - Flugzeug-ID (icao24): Der Schlüssel für das kombinierte RDD.\n",
    "        - Ein Dictionary mit:\n",
    "            - \"aircraft_info\": Ein Dictionary mit Registrierungsinformationen \n",
    "                               (Beschreibung, Hersteller, Modell, Typ, Airline).\n",
    "            - \"flight_data\": Eine Liste der Bewegungsdaten, sortiert nach Zeit.\n",
    "    \"\"\"\n",
    "    result_rdd = vehicle_rdd.leftOuterJoin(cleaned_aircraft_rdd)\n",
    "    combined_rdd = result_rdd.map(lambda x: (\n",
    "        x[0],  # Flugzeug-ID\n",
    "        {\n",
    "            \"aircraft_info\": x[1][1] if x[1][1] else {},  # Beschreibung, falls vorhanden\n",
    "            \"flight_data\": sorted(x[1][0], key=lambda entry: entry[\"time\"]) if x[1][0] else []  # Flugdaten sortiert nach Datum\n",
    "        }\n",
    "    ))\n",
    "\n",
    "    return combined_rdd\n",
    "\n",
    "\n",
    "# Beispielanwendung\n",
    "cleaned_aircraft_rdd = parse_aircraft_data(aircraft_raw_rdd)\n",
    "combined_rdd = combine_rdds(vehicle_rdd, cleaned_aircraft_rdd)\n",
    "\n",
    "# Optional: Ausgabe der Ergebnisse\n",
    "#combined_rdd.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse des zuvor kombinierten RDD\n",
    "\n",
    "- Nimmt das kombinierte RDD (Flugzeug- und Bewegungsdaten) und wendet eine Stichprobenziehung an (standardmäßig 100% der Daten).  \n",
    "- Bestimmt die Gesamtzahl aktiver Flug- und Bodenobjekte, listet die Top 5 mit den meisten Datensätzen und zeigt an, wie viele gerade fliegen bzw. am Boden sind.  \n",
    "- Analysiert außerdem, ob Objekte im gesamten Zeitraum nur am Boden, nur in der Luft oder teils geflogen sind.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "    - Skaliarbeitkeit bei verschiedenen Datenmengen analysiert (siehe ==== Untersuchung der Analysen ====)\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Teilweise Ausfälle beeinflussen nicht den Prozess\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung des Speichers: genügend Speicher für Join-Operationen und Aggregationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_combined_rdd(sampleRDD, percentage=1.0):\n",
    "    \"\"\"\n",
    "    Analysiert ein kombiniertes RDD, das Bewegungsdaten von Flug- und Bodenobjekten enthält,\n",
    "    und liefert eine detaillierte statistische Zusammenfassung der betrachteten Daten.\n",
    "\n",
    "    Args:\n",
    "    sampleRDD (RDD): Ein RDD, das Flug- und Bewegungsdaten von Objekten enthält, wobei jedes Element ein Tupel ist,\n",
    "                      wobei das erste Element die Objekt-ID ist und das zweite Element ein Dictionary mit den Bewegungsdaten.\n",
    "    percentage (float): Der Anteil der Stichprobe (zwischen 0 und 1) der RDD-Daten, die für die Analyse verwendet werden sollen. \n",
    "                        Standardwert ist 1.0 (100%).\n",
    "\n",
    "    Returns:\n",
    "    str: Eine formatierte Zeichenkette, die die Ergebnisse der Analyse zusammenfasst, einschließlich der Gesamtzahl der\n",
    "         Objekte, der Flugzeuge mit den meisten Datensätzen und der Status- und Bewegungsanalyse für den betrachteten Zeitraum.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    #Reduktion des RDDs auf Prozentwert an Datensätzen, seed sorgt für reproduzierbare Zufallsergebnisse (42 ist die Antwort auf alles).\n",
    "    combined_rdd = sampleRDD.sample(False, percentage, seed=42)\n",
    "    amount_of_aircrafts = combined_rdd.count()\n",
    "    result = \"=\" * 50 + \"\\n\"\n",
    "    result += f\"Anzahl der im betrachteten Zeitraum aktiven Flug- und Bodenobjekte: {amount_of_aircrafts}\\n\"\n",
    "    \n",
    "    #Ermittlung der Anzahl an Flug-Datensätze pro Flugzeug\n",
    "    countdatasetsRDD = combined_rdd.map(lambda x: (x[0],len(x[1]['flight_data']))).sortBy(lambda x: x[1], ascending=False)\n",
    "    first_5_elements = countdatasetsRDD.take(5)\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "    result += f\"Flugzeuge mit den meisten Datensätzen (Top 5) sind: {first_5_elements}\\n \"\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "\n",
    "\n",
    "    # Bestimmung der Positon - Skalierbarkeit gegeben über Betrachtung einzelner Werte\n",
    "    airRDD = combined_rdd.map(lambda x: (x[0], x[1]['flight_data'][-1][\"onground\"]))\n",
    "    onground = airRDD.filter(lambda x: x[1]==1).count()\n",
    "    inair = airRDD.filter(lambda x: x[1]==0).count()\n",
    "    result += f\"Status der Objekt zum letzten gemessenen Zeitpunkt: \\n\"\n",
    "    result += f\"Objekte aktuell in der Luft: {inair}\\n\"\n",
    "    result += f\"Objekte aktuell am Boden: {onground}\\n\"\n",
    "\n",
    "\n",
    "    # Ermittlung der Position im Zeitraum - Skalierbarkeit gegeben über Betrachtung einzelner Werte\n",
    "    nextRDD = combined_rdd.map(lambda x: (x[0], sum(datapoint[\"onground\"] for datapoint in x[1]['flight_data']), len(x[1]['flight_data'])))\n",
    "    calculatedRDD = nextRDD.map(lambda x: (x[0], x[1]/x[2]))\n",
    "    startet_or_landed = calculatedRDD.filter(lambda x: x[1] < 1 and x[1] >0).count()\n",
    "    not_started = calculatedRDD.filter(lambda x: x[1] == 1).count()\n",
    "    nlRDD = calculatedRDD.filter(lambda x: x[1] == 0)\n",
    "    not_landed = calculatedRDD.filter(lambda x: x[1] == 0).count()\n",
    "\n",
    "    result += \"=\" * 50 + \"\\n\"\n",
    "    result += f\"Im gesamten betrachteten Zeitraum sind: \\n\"\n",
    "    result += f\"Objekte nur am Boden geblieben: {not_started}\\n\"\n",
    "    result += f\"Objekte nur in der Luft geblieben: {not_landed}\\n\"\n",
    "    result += f\"Objekte gestartet oder gelandet: {startet_or_landed}\"\n",
    "\n",
    " \n",
    "    return combined_rdd, result\n",
    "\n",
    "combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Auswertung - Flugobjekte und Verknüpfung mit Flughäfen <a id=\"flughäfen\"></a>\n",
    "\n",
    "Identifizierung von Starts und Landungen von Flugobjekten und Verknüpfung mit nahe gelegenen Flughäfen (innerhalb von 10 km)\n",
    "\n",
    "- Berechnet mithilfe der Haversine-Formel die Distanz zwischen zwei Koordinatenpunkten.  \n",
    "- Erkennt Start- und Landemomente anhand von `onground`-Werten (`1 -> 0` und zurück).  \n",
    "- ignoriert Flüge mit einer Flugdauer < 300 Sekunden\n",
    "- Sucht zu jedem Start- und Landezeitpunkt den nächsten Flughafen (max. 10 km entfernt).  \n",
    "- Ignoriert Datensätze von Oberflächenfahrzeugen (Surface Vehicles).  \n",
    "- Gibt eine Liste mit allen erkannten Flügen inklusive Start-/Endkoordinaten und nächstgelegenen Flughäfen zurück.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Einfach zu skalieren\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Teilweise Ausfälle beeinflussen nicht den Prozess\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung des Speichers: genügend Speicher für Join-Operationen und Aggregationen\n",
    "    - Optimierung der Visualisierung: separate Ressourcen für die Generierung der Folium-Karte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime\n",
    "\n",
    "# Haversine-Funktion zum Berechnen der Distanz zwischen zwei Punkten\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Berechnet die Distanz zwischen zwei geographischen Punkten auf der Erdoberfläche\n",
    "    unter Verwendung der Haversine-Formel.\n",
    "\n",
    "    Die Funktion verwendet die Haversine-Formel zur Berechnung der kürzesten Distanz \n",
    "    auf der Erdoberfläche zwischen zwei geographischen Koordinaten (Breiten- und Längengrad).\n",
    "\n",
    "    :param lat1: Breitengrad des ersten Punkts in Dezimalgraden.\n",
    "    :param lon1: Längengrad des ersten Punkts in Dezimalgraden.\n",
    "    :param lat2: Breitengrad des zweiten Punkts in Dezimalgraden.\n",
    "    :param lon2: Längengrad des zweiten Punkts in Dezimalgraden.\n",
    "\n",
    "    :return: Die Distanz zwischen den beiden Punkten in Kilometern.\n",
    "    \"\"\"\n",
    "\n",
    "    R = 6371  # Erd-Radius in Kilometern\n",
    "    d_lat = math.radians(lat2 - lat1)\n",
    "    d_lon = math.radians(lon2 - lon1)\n",
    "    a = math.sin(d_lat / 2.0) ** 2 + math.cos(math.radians(lat1)) * math.cos(math.radians(lat2)) * math.sin(d_lon / 2.0) ** 2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "\n",
    "def find_nearest_airport(lat, lon, airports):\n",
    "    \"\"\"\n",
    "    Findet den nächsten Flughafen zu gegebenen Koordinaten innerhalb von 10 km.\n",
    "\n",
    "    Diese Funktion berechnet für gegebene geographische Koordinaten (Breiten- und Längengrad)\n",
    "    die Entfernungen zu einer Liste von Flughäfen und gibt den Flughafen mit der geringsten \n",
    "    Distanz zurück, solange die Distanz innerhalb von 10 km liegt.\n",
    "\n",
    "    :param lat: Breitengrad des aktuellen Punktes in Dezimalgraden.\n",
    "    :param lon: Längengrad des aktuellen Punktes in Dezimalgraden.\n",
    "    :param airports: Eine Liste von Flughäfen, wobei jeder Flughafen ein Tupel ist, \n",
    "                     das die Flughafen-ID, Name, und Koordinaten enthält.\n",
    "\n",
    "    :return: Ein Tupel bestehend aus der Flughafen-ID und dem Namen des nächstgelegenen Flughafens,\n",
    "             oder None, None, wenn kein Flughafen innerhalb von 10 km gefunden wird.\n",
    "    \"\"\"\n",
    "\n",
    "    nearest_airport = None\n",
    "    min_distance = float(\"inf\")\n",
    "\n",
    "    for airport in airports:\n",
    "        airport_id, _, airport_name, airport_lat, airport_lon = airport\n",
    "        airport_lat = float(airport_lat)\n",
    "        airport_lon = float(airport_lon)\n",
    "        distance = haversine(lat, lon, airport_lat, airport_lon)\n",
    "        if distance <= 10 and distance < min_distance:  # Innerhalb von 10 km und geringste Distanz\n",
    "            min_distance = distance\n",
    "            nearest_airport = (airport_id, airport_name)\n",
    "\n",
    "    if nearest_airport:\n",
    "        return nearest_airport\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def extract_flights_with_airports(record, airports):\n",
    "    \"\"\"\n",
    "    Extrahiert Flugdaten und fügt Flughafendaten hinzu.\n",
    "\n",
    "    Diese Funktion analysiert Flugbewegungsdaten, erkennt Flüge basierend auf den On-Ground-Status\n",
    "    und verknüpft sie mit den nächstgelegenen Flughäfen am Start- und Zielpunkt. \n",
    "    Es wird eine Liste von Flügen mit den zugehörigen Flughafeninformationen zurückgegeben.\n",
    "\n",
    "    :param record: Ein Tupel bestehend aus der Flugzeug-ID (icao24) und einem Dictionary \n",
    "                   mit den kombinierten RDD-Daten (Flugdaten und Flugzeuginformationen).\n",
    "    :param airports: Eine Liste von Flughäfen, die für die Bestimmung der nächsten Flughäfen \n",
    "                     verwendet wird.\n",
    "\n",
    "    :return: Eine Liste von Flügen, wobei jeder Flug ein Tupel mit der Flugzeug-ID, den \n",
    "             Flugzeuginformationen und den Flugdetails (Start- und Endzeit, Koordinaten, Flughäfen) ist.\n",
    "             Wenn der Datensatz ein \"Surface Vehicle\" enthält, wird der Flug ignoriert.\n",
    "    \"\"\"\n",
    "\n",
    "    icao24, combined_rdd = record\n",
    "    aircraft_info = combined_rdd['aircraft_info']\n",
    "    flight_data = combined_rdd['flight_data']\n",
    "\n",
    "    # Filter: Ignoriere Datensätze mit \"Surface Vehicle\" in der Beschreibung\n",
    "    if 'description' in aircraft_info and 'Surface Vehicle' in aircraft_info['description']:\n",
    "        return []\n",
    "\n",
    "    flights = []\n",
    "    current_flight = []\n",
    "    in_flight = False\n",
    "    previous_entry = None  # Variable, um den vorherigen Datensatz zu speichern\n",
    "\n",
    "    for entry in flight_data:\n",
    "        # Überprüfung, ob es sich um einen Start handelt\n",
    "        if not in_flight and entry['onground'] == 0 and previous_entry and previous_entry['onground'] == 1:\n",
    "            in_flight = True\n",
    "            current_flight = [entry]  # Startpunkt hinzufügen\n",
    "        elif in_flight and entry['onground'] == 1:  # Landung erkannt\n",
    "            current_flight.append(entry)  # Landepunkt hinzufügen\n",
    "            # Flug-Informationen extrahieren\n",
    "            start = current_flight[0]\n",
    "            end = current_flight[-1]\n",
    "\n",
    "            # Umwandlung der Zeiten in datetime-Objekte mit Korrektur der Zeitzone\n",
    "            start_time_str = start['time'].replace('+00', '')\n",
    "            end_time_str = end['time'].replace('+00', '')\n",
    "            \n",
    "            # Umwandlung der Zeiten in datetime-Objekte\n",
    "            start_time = datetime.strptime(start_time_str, \"%Y/%m/%d %H:%M:%S\")\n",
    "            end_time = datetime.strptime(end_time_str, \"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "            # Berechnung der Zeitdifferenz in Sekunden\n",
    "            time_difference = (end_time - start_time).total_seconds()\n",
    "\n",
    "            # Flug ignorieren, wenn die Zeitdifferenz unter 5 Minuten liegt\n",
    "            if time_difference < 300:\n",
    "                in_flight = False\n",
    "                current_flight = []\n",
    "                continue\n",
    "\n",
    "            start_airport_id, start_airport_name = find_nearest_airport(start['lat'], start['lon'], airports)\n",
    "            end_airport_id, end_airport_name = find_nearest_airport(end['lat'], end['lon'], airports)\n",
    "\n",
    "            flight_info = {\n",
    "                'start_time': start['time'],\n",
    "                'start_coords': (start['lat'], start['lon']),\n",
    "                'start_airport': {'id': start_airport_id, 'name': start_airport_name},\n",
    "                'end_time': end['time'],\n",
    "                'end_coords': (end['lat'], end['lon']),\n",
    "                'end_airport': {'id': end_airport_id, 'name': end_airport_name},\n",
    "            }\n",
    "            flights.append((icao24, aircraft_info, flight_info))\n",
    "            in_flight = False\n",
    "            current_flight = []\n",
    "        elif in_flight:  # Punkte während des Fluges sammeln\n",
    "            current_flight.append(entry)\n",
    "\n",
    "        # Speichere den aktuellen Eintrag als vorherigen für die nächste Iteration\n",
    "        previous_entry = entry\n",
    "\n",
    "    return flights\n",
    "\n",
    "# Airports-Daten als Liste sammeln (Broadcast-Variable verwenden)\n",
    "# Annahme: airportRDD ist das gegebene RDD mit den Flughafendaten\n",
    "airports_list = airportRDD.collect()\n",
    "broadcast_airports = sc.broadcast(airports_list)\n",
    "\n",
    "# Anwenden der Extraktionsfunktion auf die RDD mit Flughafendaten\n",
    "flights_rdd = combined_rdd.flatMap(lambda record: extract_flights_with_airports(record, broadcast_airports.value))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Verbindung zwischen Flughäfen visualisiern**\n",
    "- Aggregiert Flüge anhand ihrer Start- und Endflughäfen und zählt die Verbindungen.  \n",
    "- Erstellt daraus ein Netzwerkdiagramm (Graph) mit networkX und filtert nur Verbindungen mit mehr als 5 Flügen.  \n",
    "- Berechnet die Knotenpositionen per `spring_layout` und visualisiert das Ganze interaktiv in Plotly.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Begrenzt zu skalieren, da alle Daten müssen auf den Driver-Knoten geladen werden müssen\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Gut zu skalieren\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Daten werden über mehrere Knoten repliziert\n",
    "    - Networkx / Plotty profitieren nicht von den Fehlertoleranzmechanismen\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen\n",
    "    - Optimierung des Speichers: genügend Speicher für Join-Operationen und Aggregationen\n",
    "    - Alternativen für große Netzwerke: verteilte Graphenverarbeitungstools wie GraphX oder GraphFrames für verbesserte Skalierbarkeit/Fehlertoleranz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Beispiel-Daten erweitern: Liste von Flügen\n",
    "flights_data = flights_rdd.collect()\n",
    "\n",
    "# Schritt 1: Flugverbindungen aggregieren (Start- und End-Airport-Paare zählen)\n",
    "connections = defaultdict(int)\n",
    "start_end_names = {}\n",
    "\n",
    "for _, _, flight_info in flights_data:\n",
    "    start_airport = flight_info['start_airport']\n",
    "    end_airport = flight_info['end_airport']\n",
    "    \n",
    "    if start_airport['id'] and end_airport['id']:  # Nur valide Airports berücksichtigen\n",
    "        start_id, start_name = start_airport['id'], start_airport['name']\n",
    "        end_id, end_name = end_airport['id'], end_airport['name']\n",
    "        \n",
    "        connections[(start_id, end_id)] += 1\n",
    "        start_end_names[start_id] = start_name\n",
    "        start_end_names[end_id] = end_name\n",
    "\n",
    "# Schritt 2: Netzwerkdiagramm erstellen\n",
    "G = nx.Graph()\n",
    "\n",
    "# Knoten und Kanten hinzufügen (nur Verbindungen mit mehr als X Flügen)\n",
    "for (start, end), weight in connections.items():\n",
    "    if weight > 1:  # Filter: Nur Verbindungen > X\n",
    "        G.add_edge(start, end, weight=weight)\n",
    "\n",
    "# Schritt 3: Positionen berechnen\n",
    "pos = nx.spring_layout(G, seed=42)\n",
    "\n",
    "# Knoten und Kanten für Plotly vorbereiten\n",
    "edges_x = []\n",
    "edges_y = []\n",
    "weights = []\n",
    "\n",
    "for edge in G.edges(data=True):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    edges_x.extend([x0, x1, None])\n",
    "    edges_y.extend([y0, y1, None])\n",
    "    weights.append(edge[2]['weight'])\n",
    "\n",
    "# Knoten vorbereiten\n",
    "nodes_x = []\n",
    "nodes_y = []\n",
    "node_text = []\n",
    "\n",
    "for node in G.nodes():\n",
    "    x, y = pos[node]\n",
    "    nodes_x.append(x)\n",
    "    nodes_y.append(y)\n",
    "    node_text.append(start_end_names.get(node, node))  # Namen verwenden\n",
    "\n",
    "# Schritt 4: Interaktives Diagramm mit Plotly erstellen\n",
    "fig = go.Figure()\n",
    "\n",
    "# Kanten hinzufügen mit stärkerer Gewichtung\n",
    "max_weight = max(weights) if weights else 1  # Maximale Gewichtung für Skalierung\n",
    "\n",
    "for i, edge in enumerate(G.edges(data=True)):\n",
    "    x0, y0 = pos[edge[0]]\n",
    "    x1, y1 = pos[edge[1]]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=[x0, x1, None],\n",
    "        y=[y0, y1, None],\n",
    "        line=dict(width=(weights[i] / max_weight) * 10, color='black'),  # Stärkere Skalierung (100-fach)\n",
    "        hoverinfo='none',\n",
    "        mode='lines'))\n",
    "\n",
    "# Knoten hinzufügen\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=nodes_x, y=nodes_y,\n",
    "    mode='markers+text',\n",
    "    text=node_text,\n",
    "    textposition=\"top center\",\n",
    "    marker=dict(\n",
    "        size=10,\n",
    "        color='skyblue',\n",
    "        line_width=2),\n",
    "    hoverinfo='text'))\n",
    "\n",
    "# Layout anpassen\n",
    "fig.update_layout(\n",
    "    title=\"Netzwerkdiagramm der Flugverbindungen\",\n",
    "    title_x=0.5,\n",
    "    showlegend=False,\n",
    "    hovermode='closest',\n",
    "    margin=dict(b=0, l=0, r=0, t=30),\n",
    "    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n",
    "    plot_bgcolor='white'\n",
    ")\n",
    "\n",
    "# Interaktives Diagramm anzeigen\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Auswertung - Visualisierung ausgewählter Bewegungsdaten\n",
    "- Wählt eine bestimmte Flugzeug-Route aus anhand der (`selected_icao`) und zeichnet sie auf einer Folium-Karte.  \n",
    "- Für das Flugzeug wird die Route zwischen aufeinanderfolgenden Punkten eingezeichnet.  \n",
    "- Landepunkte (``onground == 1``) werden als grüne Marker mit Informationen über Hersteller, Modell, Airline etc. versehen.  \n",
    "- Die Karte wird schließlich zentriert auf den ersten Punkt des ersten Flugzeugs.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Begrenzt zu skalieren, da alle Daten auf Driver-Knoten geladen werden müssen\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Begrenzt zu skalieren, Berechnen der PolyLines erfordert CPU-Ressourcen auf dem Driver.\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Folium läuft auf dem Drive Knoten ist nicht fehlertolerant\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen, gerade Speicher und CPU Auslastung auf Drive Knoten\n",
    "    - Optimierung der Visualisierung: auf separate Prozesse ausgelagert werden, um die Performance des Driver-Knotens zu verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import display\n",
    "\n",
    "def analyze_aircraft_route(selected_icao):\n",
    "    \"\"\"\n",
    "    Analysiert die Flugroute eines spezifischen Flugzeugs und erstellt eine interaktive Karte mit der Route und den Landepunkten.\n",
    "\n",
    "    Diese Funktion filtert die Bewegungsdaten basierend auf der angegebenen Flugzeug-ID (ICAO24-Code) und visualisiert die Flugroute \n",
    "    auf einer interaktiven Karte. Die Karte zeigt Linien zwischen den aufeinanderfolgenden Positionspunkten des Flugzeugs und Marker \n",
    "    für die Landepunkte an. Weitere Details zu dem Flugzeug (wie Hersteller, Modell und Airline) werden in den Markern angezeigt.\n",
    "\n",
    "    :param selected_icao: Der ICAO24-Code des Flugzeugs, dessen Route analysiert werden soll.\n",
    "    \n",
    "    :return: Eine interaktive `folium.Map`-Karte, die die Flugroute und die Landepunkte des ausgewählten Flugzeugs anzeigt.\n",
    "            Wenn keine Daten für das angegebene Flugzeug gefunden werden, gibt die Funktion eine Fehlermeldung als String zurück.\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_data = combined_rdd.filter(lambda x: x[0] == selected_icao).collect()\n",
    "\n",
    "    if filtered_data:\n",
    "        # Hole die Daten für das ausgewählte Flugzeug\n",
    "        icao24, route_data = filtered_data[0]\n",
    "        aircraft_info = route_data[\"aircraft_info\"]  # Zugriff auf die Flugzeugbeschreibung\n",
    "        flight_data = route_data[\"flight_data\"]  # Zugriff auf die Flugdaten\n",
    "\n",
    "        print(f\"Flugzeug-ID: {icao24}\")\n",
    "\n",
    "        # Erstelle die Karte, zentriert auf den ersten Datenpunkt\n",
    "        first_lat = flight_data[0][\"lat\"]\n",
    "        first_lon = flight_data[0][\"lon\"]\n",
    "        map = folium.Map(location=[first_lat, first_lon], zoom_start=12)\n",
    "\n",
    "        # Zeichne Linien zwischen aufeinanderfolgenden Punkten\n",
    "        for i in range(len(flight_data) - 1):\n",
    "            point_a = flight_data[i]\n",
    "            point_b = flight_data[i + 1]\n",
    "\n",
    "            # Zeichne nur Linien, wenn beide Punkte gültige Koordinaten haben\n",
    "            if point_a[\"lat\"] and point_a[\"lon\"] and point_b[\"lat\"] and point_b[\"lon\"]:\n",
    "                folium.PolyLine(\n",
    "                    locations=[(point_a[\"lat\"], point_a[\"lon\"]), (point_b[\"lat\"], point_b[\"lon\"])],\n",
    "                    color=\"blue\",\n",
    "                    weight=2.5,\n",
    "                    opacity=0.8\n",
    "                ).add_to(map)\n",
    "\n",
    "        # Füge Marker nur für Landepunkte hinzu\n",
    "        for point in flight_data:\n",
    "            if point[\"onground\"] == 1:  # Nur Landepunkte anzeigen\n",
    "                folium.Marker(\n",
    "                    location=(point[\"lat\"], point[\"lon\"]),\n",
    "                    popup=f\"Zeit: {point['time']}<br>Am Boden: {point['onground']}<br>Hersteller: {aircraft_info['manufacturer']}<br>Flugzeugtyp: {aircraft_info['model']}<br>Airline: {aircraft_info['airline']}\",\n",
    "                    icon=folium.Icon(color=\"green\", icon=\"info-sign\")\n",
    "                ).add_to(map)\n",
    "\n",
    "        # Rückgabe der Karte\n",
    "        return map\n",
    "\n",
    "    else:\n",
    "        return f\"Keine Daten für Flugzeug-ID '{selected_icao}' gefunden.\"\n",
    "\n",
    "\n",
    "result = analyze_aircraft_route(\"a1cf48\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.8 Auswertung - Visualisierung zufälliger Bewegungsdaten <a id=\"ausgewählt\"></a>\n",
    "\n",
    "- Wählt zufällig eine bestimmte Anzahl an Flugzeug-Routen aus (`number_of_samples`) und zeichnet sie auf einer Folium-Karte.  \n",
    "- Pro Flugzeug wird die Route zwischen aufeinanderfolgenden Punkten in einer zufällig generierten Farbe eingezeichnet.  \n",
    "- Landepunkte (``onground == 1``) werden als grüne Marker mit Informationen über Hersteller, Modell, Airline etc. versehen.  \n",
    "- Die Karte wird schließlich zentriert auf den ersten Punkt des ersten Flugzeugs.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Begrenzt zu skalieren, da alle Daten auf Driver-Knoten geladen werden müssen\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Begrenzt zu skalieren, Berechnen der PolyLines erfordert CPU-Ressourcen auf dem Driver.\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Schwer zu skalieren\n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Folium läuft auf dem Drive Knoten ist nicht fehlertolerant\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen, gerade Speicher und CPU Auslastung auf Drive Knoten\n",
    "    - Optimierung der Visualisierung: auf separate Prozesse ausgelagert werden, um die Performance des Driver-Knotens zu verbessern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from IPython.display import display\n",
    "import random\n",
    "\n",
    "# Zufällige Farben für die Flugzeuge\n",
    "def get_random_color():\n",
    "    \"\"\"\n",
    "    Generiert eine zufällige Hex-Farbe.\n",
    "\n",
    "    Diese Funktion erzeugt eine zufällige Farbe im Hexadezimalformat (z.B. '#a3c113'),\n",
    "    die für die Visualisierung von Strecken oder Markern verwendet werden kann.\n",
    "\n",
    "    :return: Eine zufällige Farbe im Hexadezimalformat als String (z.B. '#a3c113').\n",
    "    \"\"\"\n",
    "        \n",
    "    return \"#{:06x}\".format(random.randint(0, 0xFFFFFF))\n",
    "\n",
    "def analyze_aircraft_routes(number_of_samples):\n",
    "    \"\"\"\n",
    "    Analysiert die Routen einer zufälligen Auswahl von Flugzeugen und erstellt eine interaktive Karte.\n",
    "\n",
    "    Diese Funktion wählt eine angegebene Anzahl zufälliger Flugzeuge aus den Bewegungsdaten aus \n",
    "    und visualisiert deren Flugrouten auf einer interaktiven Karte. Die Karte zeigt Linien \n",
    "    zwischen den aufeinanderfolgenden Punkten jedes Flugzeugs, sowie Marker für Landepunkte. \n",
    "    Jeder Flugzeugroute wird eine zufällige Farbe zugewiesen, und die Marker beinhalten detaillierte \n",
    "    Informationen zu Hersteller, Modell und Airline des Flugzeugs.\n",
    "\n",
    "    :param number_of_samples: Die Anzahl der zufällig auszuwählenden Flugzeuge.\n",
    "    \n",
    "    :return: Eine interaktive `folium.Map`-Karte, die die Routen der zufällig ausgewählten Flugzeuge anzeigt.\n",
    "            Wenn keine Daten gefunden werden, gibt die Funktion eine Fehlermeldung als String zurück.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nimm eine beliebige Anzahl zufälliger Flugzeuge aus dem RDD\n",
    "    random_aircrafts = combined_rdd.takeSample(withReplacement=False, num=number_of_samples)\n",
    "\n",
    "    # Erstelle eine Karte, zentriert auf einen Beispielpunkt\n",
    "    if random_aircrafts:\n",
    "        # Hole den ersten Punkt zur Zentrierung der Karte\n",
    "        first_lat = random_aircrafts[0][1][\"flight_data\"][0][\"lat\"]\n",
    "        first_lon = random_aircrafts[0][1][\"flight_data\"][0][\"lon\"]\n",
    "        map = folium.Map(location=[first_lat, first_lon], zoom_start=6)\n",
    "\n",
    "        # Iteriere über die zufälligen Flugzeuge\n",
    "        for icao24, data in random_aircrafts:\n",
    "            aircraft_info = data[\"aircraft_info\"]  # Infos über das Flugzeug\n",
    "            flight_data = data[\"flight_data\"]      # Flugdaten\n",
    "\n",
    "            # Sichere Abfrage von Typ, Modell und Hersteller mit Standardwerten\n",
    "            aircraft_type = aircraft_info.get(\"type\", \"Unknown\")\n",
    "            aircraft_model = aircraft_info.get(\"model\", \"Unknown\")\n",
    "            aircraft_manufacturer = aircraft_info.get(\"manufacturer\", \"Unknown\")\n",
    "            aircraft_airline = aircraft_info.get(\"airline\", \"Unknown\")\n",
    "\n",
    "            # Generiere eine zufällige Farbe für die Linien\n",
    "            color = get_random_color()\n",
    "\n",
    "            # Zeichne Linien zwischen aufeinanderfolgenden Punkten\n",
    "            for i in range(len(flight_data) - 1):\n",
    "                point_a = flight_data[i]\n",
    "                point_b = flight_data[i + 1]\n",
    "\n",
    "                # Zeichne nur Linien, wenn beide Punkte gültige Koordinaten haben\n",
    "                if point_a[\"lat\"] and point_a[\"lon\"] and point_b[\"lat\"] and point_b[\"lon\"]:\n",
    "                    folium.PolyLine(\n",
    "                        locations=[(point_a[\"lat\"], point_a[\"lon\"]), (point_b[\"lat\"], point_b[\"lon\"])],\n",
    "                        color=color,\n",
    "                        weight=2.5,\n",
    "                        opacity=0.8\n",
    "                    ).add_to(map)\n",
    "\n",
    "            # Füge Marker nur für Landepunkte hinzu\n",
    "            for point in flight_data:\n",
    "                if point[\"onground\"] == 1:  # Nur Landepunkte anzeigen\n",
    "                    folium.Marker(\n",
    "                        location=(point[\"lat\"], point[\"lon\"]),\n",
    "                        popup=(\n",
    "                            f\"Flugzeug: {icao24}<br>\"\n",
    "                            f\"Hersteller: {aircraft_manufacturer}<br>\"\n",
    "                            f\"Modell: {aircraft_model}<br>\"\n",
    "                            f\"Airline: {aircraft_airline}<br>\"\n",
    "                            f\"Zeit: {point['time']}<br>\"\n",
    "                            f\"Am Boden: {point['onground']}\"\n",
    "                        ),\n",
    "                        icon=folium.Icon(color=\"green\", icon=\"info-sign\")\n",
    "                    ).add_to(map)\n",
    "\n",
    "        # Zeige die Karte im Notebook\n",
    "        return map\n",
    "    else:\n",
    "        return \"Keine Daten gefunden.\"\n",
    "    \n",
    "result = analyze_aircraft_routes(1)\n",
    "display(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.9 Auswertung - Heatmap der Bewegungsdaten <a id=\"heatmap\"></a>\n",
    "\n",
    "- Extrahiert Koordinaten (Breiten- und Längengrad)** aus dem Eingabe-RDD und rundet sie auf eine bestimmte Genauigkeit (hier `round(...)` mit einer Nachkommastelle).  \n",
    "- Aggregiert die Häufigkeiten jeder Koordinatenkombination, um beispielsweise Cluster zu identifizieren.  \n",
    "- Erstellt abschließend eine Heatmap mit Folium basierend auf den gezählten Häufigkeiten.\n",
    "\n",
    "Diskussion:\n",
    "- Skalierbarkeit\n",
    "    - Verarbeitungslaufzeit wächst proportional zur Datenmenge\n",
    "    - Skaliarbeitkeit bei verschiedenen Datenmengen analysiert (siehe ==== Untersuchung der Analysen ====)\n",
    "- Datenfluss\n",
    "    - IO-gebunden: Einlesen der Daten aus dem Dateisystem → Gut skalierbar durch paralleles Einlesen und Verarbeiten auf mehreren Knoten\n",
    "    - CPU-gebunden: Parsen und Filtern der CSV-Daten → Begrenzt zu skalieren, Berechnen der PolyLines erfordert CPU-Ressourcen auf dem Driver.\n",
    "    - Memory-gebunden: Aggregationen und Sortierungen im Speicher → Eingeschränkt zu skalieren \n",
    "    - Visualisierung mit Plotly: das Rendern der Diagramme erfordert viel Speicher und CPU -> Begrenzt zu skalieren\n",
    "- Fehlertoleranz:\n",
    "    - RDDs ermöglichen automatische Wiederherstellung bei Ausfällen\n",
    "    - Folium läuft auf dem Drive Knoten ist nicht fehlertolerant\n",
    "- Optimiert für Batch-Verarbeitung, nicht für Echtzeitanalysen -> Verarbeitung erfolgt in großen Blöcken\n",
    "- Dimensionierung eines realen Systems:\n",
    "    - Ausreichend Knoten mit ausreichendem CPU- und Speicher\n",
    "    - Kontinuierliches Monitoring und Anpassungen basierend auf Leistungskennzahlen, gerade Speicher und CPU Auslastung auf Drive Knoten\n",
    "    - Optimierung der Visualisierung: auf separate Prozesse ausgelagert werden, um die Performance des Driver-Knotens zu verbessern -> Datenmenge begrenzen, weitere Filterung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "\n",
    "\n",
    "def process_and_reduce(dataRDD, percentage=1.0):\n",
    "    \"\"\"\n",
    "    Verarbeitet das Eingabe-RDD, extrahiert die Koordinaten (Breitengrad, Längengrad) \n",
    "    und reduziert die Daten, indem die Häufigkeit jedes Koordinatenpaars gezählt wird.\n",
    "    Gibt das reduzierte RDD der Koordinaten und deren Häufigkeiten zurück.\n",
    "\n",
    "    Diese Funktion bereinigt die Eingabedaten, indem sie ungültige Koordinaten entfernt und die \n",
    "    Häufigkeit jedes Koordinatenpaars zählt. Es wird die Möglichkeit geboten, nur einen Prozentsatz \n",
    "    der Daten zu verwenden, was die Verarbeitung von großen Datensätzen effizienter gestaltet.\n",
    "\n",
    "    :param dataRDD: Das Eingabe-RDD, das Bewegungsdaten von Flugzeugen oder Objekten enthält. \n",
    "    :param percentage: Prozentsatz der Daten, die verarbeitet werden sollen (Standardwert: 100%).\n",
    "\n",
    "    :return: Ein RDD mit den Koordinatenpaaren und deren Häufigkeiten sowie eine Ergebniszusammenfassung als String.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extrahiere Koordinaten\n",
    "    def extract_coordinates(line):\n",
    "        try:\n",
    "            parts = line.split(',')\n",
    "            return round(float(parts[2]), 1), round(float(parts[3]), 1)  # Breitengrad, Längengrad\n",
    "        except (ValueError, IndexError):\n",
    "            return None  # Falls Konvertierung oder Zugriff fehlschlägt\n",
    "\n",
    "    # Reduktion der Daten\n",
    "    cleaned_rdd = dataRDD.map(extract_coordinates).filter(lambda x: x is not None)\n",
    "\n",
    "    # Zähle, wie oft jede Kombination von Breitengrad und Längengrad vorkommt\n",
    "    reduced_rdd = cleaned_rdd.map(lambda coord: (coord, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "    result = f\"Anzahl der Daten vor der Reduktion: {dataRDD.count()}\\n\"\n",
    "    result += f\"Anzahl Koordinaten nach der Reduktion: {reduced_rdd.count()}\\n\"\n",
    "\n",
    "    return reduced_rdd, result\n",
    "\n",
    "\n",
    "def create_heatmap(reduced_rdd):\n",
    "    \"\"\"\n",
    "    Erstellt eine Heatmap aus den aggregierten Koordinaten und deren Häufigkeiten.\n",
    "\n",
    "    Diese Funktion nimmt das reduzierte RDD der Koordinaten und deren Häufigkeiten \n",
    "    und erstellt eine interaktive Heatmap, die die Dichte der Koordinaten anzeigt. \n",
    "    Die Farbe und Intensität der Heatmap-Punkte basiert auf der Häufigkeit der Koordinatenpaare.\n",
    "\n",
    "    :param reduced_rdd: Ein RDD, das Koordinatenpaare und deren Häufigkeit enthält.\n",
    "\n",
    "    :return: Eine `folium.Map`-Karte mit der Heatmap der Koordinaten, die die Dichte der Bewegungen anzeigt.\n",
    "             Gibt eine Fehlermeldung aus, wenn keine gültigen Punkte vorhanden sind.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Beispiel: Aggregierte Koordinaten mit Häufigkeiten (ersetzt durch deine Daten)\n",
    "    reduced_data = reduced_rdd.collect()\n",
    "\n",
    "    # Vorbereitung der Heatmap-Daten\n",
    "    heatmap_data = [(lat_lon[0], lat_lon[1], count) for lat_lon, count in reduced_data]\n",
    "\n",
    "    # Erstelle die Karte\n",
    "    if heatmap_data:\n",
    "        # Zentriere die Karte auf den ersten Punkt und passe die Zoom-Stufe an\n",
    "        map = folium.Map(location=[heatmap_data[0][0], heatmap_data[0][1]], zoom_start=6)\n",
    "\n",
    "        # HeatMap hinzufügen (Radius und max_zoom können angepasst werden)\n",
    "        HeatMap(heatmap_data, radius=15, max_zoom=13).add_to(map)\n",
    "        \n",
    "        return map\n",
    "    \n",
    "    else:\n",
    "        print(\"Keine gültigen Punkte für die Heatmap gefunden.\")\n",
    "\n",
    "\n",
    "\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "print(result)\n",
    "map = create_heatmap(reduced_rdd)\n",
    "display(map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Analyse der Auswertungen <a id=\"analyse\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Definition der Analyse-Funktionen <a id=\"funktionen\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Datenverteilung in RDDs:\n",
    "  Bestimmt die Anzahl der Datensätze pro Partition sowie die Gesamtzahl der Partitionen und Datensätze.\n",
    "\n",
    "- Fehlertoleranz im Cluster:\n",
    "  Die Funktion simuliert den Ausfall einer Worker-Node, führt die Analyse mit und ohne Ausfall durch und protokolliert die Ergebnisse oder Fehler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psutil\n",
    "import time\n",
    "from multiprocessing import Process\n",
    "import psutil\n",
    "import requests\n",
    "\n",
    "# App ID aus Cluster extrahieren\n",
    "def get_app_id(master_url):\n",
    "    \"\"\"Ermittelt die aktuelle Application ID vom Spark Web UI.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{master_url}/api/v1/applications\")\n",
    "        if response.status_code == 200:\n",
    "            applications = response.json()\n",
    "            if applications:\n",
    "                return applications[0][\"id\"]  # Erste aktive oder abgeschlossene App\n",
    "            else:\n",
    "                print(\"Keine Anwendungen gefunden.\")\n",
    "        else:\n",
    "            print(f\"Fehler beim Abrufen der Anwendungen: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Ermitteln der Application ID: {e}\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Datenverteilung im Cluster\n",
    "def analyze_partition_distribution(rdd):\n",
    "    # Funktion direkt im Mapping definieren\n",
    "    def count_records(index, iterator):\n",
    "        return [(socket.gethostname(), index, sum(1 for _ in iterator))]\n",
    "\n",
    "    import socket\n",
    "    \n",
    "    # Sammle alle Informationen\n",
    "    all_partitions = rdd.mapPartitionsWithIndex(count_records).collect()\n",
    "    # Ausgabe vorbereiten\n",
    "    print(\"\\n=== Partition Verteilung ===\")\n",
    "    \n",
    "    # Gruppiere nach Hostname\n",
    "    by_host = {}\n",
    "    for hostname, part_id, records in all_partitions:\n",
    "        if hostname not in by_host:\n",
    "            by_host[hostname] = []\n",
    "        by_host[hostname].append((part_id, records))\n",
    "    \n",
    "    # Ausgabe formatieren\n",
    "    for hostname, partitions in by_host.items():\n",
    "        total_records = sum(records for _, records in partitions)\n",
    "        print(f\"\\nWorker: {hostname}\")\n",
    "        print(f\"├── Anzahl Partitionen: {len(partitions)}\")\n",
    "        print(f\"├── Gesamt Records: {total_records}\")\n",
    "        print(\"└── Details:\")\n",
    "        for part_id, records in sorted(partitions):\n",
    "            print(f\"    ├── Partition {part_id}: {records} Records\")\n",
    "        \n",
    "\n",
    "# Fehlertoleranz bei Beenden einer Worker Node\n",
    "\n",
    "def simulate_worker_failure(analyze_func, data_path, worker_pids):\n",
    "    \"\"\"Führt Analyse durch und lässt einen Worker ausfallen\"\"\"\n",
    "    # Erste Analyse\n",
    "    print(\"\\nErste Analyse startet...\")\n",
    "    start = time.time()\n",
    "    analyze_func(data_path)\n",
    "    time1 = time.time() - start\n",
    "    print(f\"Erste Analyse: {time1:.1f} Sekunden\")\n",
    "\n",
    "    # Worker auswählen und beenden\n",
    "    print(\"\\nWorker PIDs:\", worker_pids)\n",
    "    choice = input(\"Welchen Worker beenden? Gibt eine Zahl von 1 - \" + str(len(worker_pids)) + \" an. Möchtest du nichts beenden gebe 'n' ein!\")\n",
    "    \n",
    "    if choice != 'n':\n",
    "        pid = worker_pids[int(choice)-1]\n",
    "        \n",
    "        # Prozess mit psutil beenden\n",
    "        try:\n",
    "            proc = psutil.Process(pid)\n",
    "            proc.terminate()  # Alternativ: proc.kill() für sofortiges Beenden\n",
    "            print(f\"Worker {pid} beendet.\")\n",
    "            worker_pids.remove(pid)\n",
    "        except psutil.NoSuchProcess:\n",
    "            print(f\"Prozess {pid} wurde bereits beendet.\")\n",
    "        \n",
    "        time.sleep(2)  # Kurz warten\n",
    "    if len(worker_pids) == 0:\n",
    "        print(\"Abbruch - Keine Worker Nodes verfügbar!\")\n",
    "        return worker_pids\n",
    "    else:\n",
    "\n",
    "        # Zweite Analyse\n",
    "        print(\"\\nZweite Analyse startet...\")\n",
    "        start = time.time()\n",
    "        analyze_func(data_path)\n",
    "        time2 = time.time() - start\n",
    "        print(f\"Zweite Analyse: {time2:.1f} Sekunden\")\n",
    "        print(\"Analyse erfolgreich durchgeführt!\")\n",
    "\n",
    "        return worker_pids\n",
    "\n",
    "def get_executor_metrics():\n",
    "\n",
    "    # RAM-Auslastung\n",
    "    memory = psutil.virtual_memory()\n",
    "    ram_usage_percent = memory.percent  # RAM-Auslastung in Prozent\n",
    "\n",
    "    # CPU-Auslastung\n",
    "    cpu_usage_percent = psutil.cpu_percent(interval=1)  # CPU-Auslastung in Prozent\n",
    "\n",
    "    return ram_usage_percent, cpu_usage_percent\n",
    "\n",
    "# Visualisierung der Skalierbarkeit in einem Chart\n",
    "def plot_results(x_data, y_time, memory_data, cpu_data):\n",
    "    \"\"\"Visualisiert die Ergebnisse in einem gemeinsamen Plot.\"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    # Verarbeitungszeit\n",
    "    ax1.set_xlabel('Datenmenge (%)')\n",
    "    ax1.set_ylabel('Laufzeit (Sekunden)', color='b')\n",
    "    ax1.plot(x_data, y_time, marker='o', linestyle='-', color='b', label='Laufzeit')\n",
    "    ax1.tick_params(axis='y', labelcolor='b')\n",
    "\n",
    "    # Zweite Y-Achse für RAM (in Prozent)\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('RAM-Auslastung (%)', color='r')  # Hier haben wir % für RAM\n",
    "    ax2.plot(x_data, memory_data, marker='o', linestyle='-', color='r', label='RAM-Auslastung (%)')\n",
    "    ax2.tick_params(axis='y', labelcolor='r')\n",
    "\n",
    "    # Dritte Y-Achse für CPU (in Prozent)\n",
    "    ax3 = ax1.twinx()\n",
    "    ax3.spines[\"right\"].set_position((\"outward\", 60))  # Platz für die dritte Y-Achse\n",
    "    ax3.set_ylabel('CPU-Auslastung (%)', color='y')  # Hier auch % für CPU\n",
    "    ax3.plot(x_data, cpu_data, marker='o', linestyle='-', color='y', label='CPU-Auslastung (%)')\n",
    "    ax3.tick_params(axis='y', labelcolor='y')\n",
    "\n",
    "    # Titel und Layout\n",
    "    fig.suptitle('Skalierbarkeitsanalyse: Laufzeit, RAM und CPU-Auslastung', fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Analyse - Auswertung - Weltweite Flughäfen\n",
    "\n",
    "- Laufzeit: Erfasst die Ausführungsdauer der Funktion `analyze_airports(...)` mithilfe von `time.time()`.  \n",
    "- Datenverteilung: Ruft `analyze_partition_distribution(rdd)` auf und zeigt, wie die im RDD enthaltenen Daten über das Cluster hinweg partitioniert sind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laufzeit**\n",
    "- Durchführung der Funktion mit einer minimalen Teilmenge der Daten, um eine Basismessung der benötigten Zeit zu erhalten.\n",
    "- Ausgabe der Laufzeit bei der Verarbeitung aller Daten, abzüglich der Basiszeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Laufzeit\n",
    "start_time = time.time()\n",
    "analyze_airports(airport_data_path)\n",
    "end_time = time.time()\n",
    "print(f\"Die Funktion hat {(end_time - start_time):.4f} Sekunden benötigt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datenverteilung**\n",
    "- Bestimmung der Gesamtzahl der Datenpunkte im Dataset.\n",
    "- Untersuchung der Anzahl der Partitionen sowie der Verteilung der Daten über diese Partitionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = analyze_partition_distribution(airportRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Analyse - Registrierte Flugobjekte\n",
    "\n",
    "- Laufzeit: Stoppt die Zeit vor und nach dem Aufruf der Funktion `analyze_aircraft_data(...)`, um die Ausführungsdauer zu bestimmen.  \n",
    "- Datenverteilung: Ruft `analyze_partition_distribution(aircraftRDD)` auf und zeigt an, wie viele Datensätze es insgesamt gibt, wie viele Partitionen vorliegen und wie die Datensätze auf die Partitionen verteilt sind."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laufzeit**\n",
    "- Durchführung der Funktion mit einer minimalen Teilmenge der Daten, um eine Basismessung der benötigten Zeit zu erhalten.\n",
    "- Ausgabe der Laufzeit bei der Verarbeitung aller Daten, abzüglich der Basiszeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "#Laufzeit\n",
    "start_time = time.time()\n",
    "analyze_aircraft_data(aircraft_data_path)\n",
    "end_time = time.time()\n",
    "print(f\"Die Funktion hat {(end_time - start_time):.4f} Sekunden benötigt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datenverteilung**\n",
    "- Bestimmung der Gesamtzahl der Datenpunkte im Dataset.\n",
    "- Untersuchung der Anzahl der Partitionen sowie der Verteilung der Daten über diese Partitionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aircraftRDD = aircraftRDD.repartition(1) # Test für Neuverteilung der Daten\n",
    "analyze_partition_distribution(aircraftRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Analyse- Bewegungsdaten von Flugobjekten\n",
    "\n",
    "- Laufzeit: Führt die Funktion `analyze_flight_movements` zunächst mit einem sehr kleinen Prozentsatz (0,01%) und allen Daten (100%) aus und misst die dafür benötigte Zeit (Basismessung).  \n",
    "- Datenverteilung: Gibt zuletzt die Anzahl an Datensätzen und deren Verteilung auf die einzelnen Partitionen aus.\n",
    "- Fehlertoleranz: Analysiert, wie sich die Funktion bei Aufall einer Worker Node im Cluster verhält.\n",
    "- Skalierbarkeit: Untersucht die Performance des Clusters (Laufzeit der Analyse) in Abhängigkeit der analysierten Datenmenge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laufzeit**\n",
    "- Durchführung der Funktion mit einer minimalen Teilmenge der Daten, um eine Basismessung der benötigten Zeit zu erhalten.\n",
    "- Ausgabe der Laufzeit bei der Verarbeitung aller Daten, abzüglich der Basiszeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Basiszeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=0.0001)\n",
    "virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "used_memory = virtual_memory.used  # Genutzter RAM\n",
    "memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "# Basiszeit errechnen\n",
    "base_time = end_time - start_time\n",
    "\n",
    "# Laufzeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktion aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=1.0)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Die Funktion hat für 100% der Daten {(end_time - start_time - base_time):.4f} Sekunden benötigt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datenverteilung**\n",
    "- Bestimmung der Gesamtzahl der Datenpunkte im Dataset.\n",
    "- Untersuchung der Anzahl der Partitionen sowie der Verteilung der Daten über diese Partitionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datenverteilung\n",
    "analyze_partition_distribution(dataRDD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fehlertoleranz**\n",
    "- Durchführung einer Analyse in Spark Cluster.\n",
    "- Optional: Händisches beenden (\"killen\") einer Worker Node im Cluster.\n",
    "- Ausgabe eines Protokolls, dass die benötigte Zeit der Analyse zeigt.\n",
    "- Cluster wird neugestartet falls eine Node bei der Analyse der Fehlertoleranz beendet wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pids = simulate_worker_failure(analyze_flight_movements, flight_data_path, worker_pids)\n",
    "\n",
    "if len(worker_pids) != WORKER_INSTANCES:\n",
    "    print(\"\")\n",
    "    print(\"==== Cluster neustarten mit allen Nodes ====\")\n",
    "    worker_pids = setup_spark_cluster(\n",
    "        worker_count=WORKER_INSTANCES,\n",
    "        worker_cores=WORKER_CORES,\n",
    "        worker_memory=WORKER_MEMORY\n",
    "    )\n",
    "\n",
    "    spark.stop()\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # Versuch, den SparkContext zu erstellen\n",
    "    try:\n",
    "        sc = spark.sparkContext  # Verwende den SparkContext aus der SparkSession\n",
    "        print(\"SparkContext erfolgreich erstellt.\")\n",
    "        print(f\"Spark läuft auf Master: {sc.master}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen des SparkContext: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skalierbarkeit**\n",
    "\n",
    "- Skalierbarkeitstest**: Untersucht, wie sich die Verarbeitung von Flugbewegungsdaten mit zunehmendem Datenvolumen verhält.\n",
    "- Datenmengen definieren**: Verarbeitet verschiedene Anteile der Daten (10%, 20%, 50%, 100%).\n",
    "- Leistungsmetriken erfassen**:\n",
    "  - Laufzeit: Misst die Ausführungszeit der Funktion `analyze_flight_movements` für jede Datenmenge.\n",
    "  - Datenverteilung: Analysiert die Verteilung der Daten über die Spark-Partitionen.\n",
    "  - Speicherauslastung: Erfasst die RAM-Auslastung nach der Verarbeitung.\n",
    "  - CPU-Auslastung: Misst die CPU-Auslastung unmittelbar nach der Verarbeitung.\n",
    "- Datenvisualisierung: Erstellt vier Diagramme zur Darstellung der gesammelten Leistungsmetriken in Abhängigkeit von der verarbeiteten Datenmenge:\n",
    "  1. Verarbeitungszeit vs. Datenmenge (%)\n",
    "  2. Anzahl der Partitionen** vs. Datenmenge (%)\n",
    "  3. RAM-Auslastung vs. Datenmenge (%)\n",
    "  4. CPU-Auslastung vs. Datenmenge (%)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Skalierbarkeit\n",
    "\n",
    "# Definition der zu verarbeitenden Datenmenge\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "\n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktion aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    \n",
    "    # Cluster-Metriken abrufen\n",
    "    total_memory, total_cpu = get_executor_metrics()\n",
    "\n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Daten für den Plot sammeln\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time - start_time)  # Verarbeitungszeit\n",
    "    memory_data.append(total_memory)  # Speicherauslastung in %\n",
    "    cpu_data.append(total_cpu)  # CPU-Auslastung in %\n",
    "\n",
    "plot_results(x_data, y_time, memory_data, cpu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Analyse - Kombination Flugobjekte und Bewegungsdaten\n",
    "\n",
    "- Laufzeit:\n",
    "  - Führt die Funktionen `analyze_flight_movements`, `restructure_flight_movements`, `analyze_aircraft_data`, `combine_rdds` und `analyze_combined_rdd` mit einem sehr kleinen Datenanteil (`0.0001%`) aus.\n",
    "  - Misst die dafür benötigte Zeit, um eine Basiszeit (`base_time`) zu bestimmen.\n",
    "  \n",
    "- Laufzeitmessung:\n",
    "  - Führt dieselben Funktionen mit `100%` der Daten aus.\n",
    "  - Misst die gesamte Ausführungszeit und subtrahiert die zuvor ermittelte Basiszeit, um die tatsächliche Verarbeitungszeit zu berechnen.\n",
    "  \n",
    "- Datenverteilung:\n",
    "  - Analysiert die Verteilung der Daten im kombinierten RDD über die verschiedenen Partitionen mithilfe der Funktion `analyze_partition_distribution`.\n",
    "  \n",
    "- Ausgabe:\n",
    "  - Gibt die berechnete Laufzeit für die Verarbeitung der vollständigen Datenmenge sowie die Ergebnisse der Datenverteilungsanalyse aus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laufzeit**\n",
    "- Durchführung der Funktion mit einer minimalen Teilmenge der Daten, um eine Basismessung der benötigten Zeit zu erhalten.\n",
    "- Ausgabe der Laufzeit bei der Verarbeitung aller Daten, abzüglich der Basiszeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Basiszeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time() \n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=0.0001)\n",
    "vehicle_rdd = restructure_flight_movements(dataRDD)\n",
    "result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd, 0.0001) #0.01% der Daten für Basiszeit\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "#Basiszeit errechnen\n",
    "base_time = end_time-start_time\n",
    "\n",
    "\n",
    "# Laufzeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "vehicle_rdd = restructure_flight_movements(dataRDD)    \n",
    "result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Die Funktion hat für 100% der Daten {end_time - start_time:.4f} Sekunden benötigt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datenverteilung**\n",
    "- Bestimmung der Gesamtzahl der Datenpunkte im Dataset.\n",
    "- Untersuchung der Anzahl der Partitionen sowie der Verteilung der Daten über diese Partitionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_partition_distribution(combined_analyzed_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fehlertoleranz**\n",
    "- Durchführung einer Analyse in Spark Cluster.\n",
    "- Optional: Händisches beenden (\"killen\") einer Worker Node im Cluster.\n",
    "- Ausgabe eines Protokolls, dass die benötigte Zeit sowie die Ergebnisse der Analyse zeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pids = simulate_worker_failure(analyze_flight_movements, flight_data_path, worker_pids)\n",
    "\n",
    "if len(worker_pids) != WORKER_INSTANCES:\n",
    "    print(\"\")\n",
    "    print(\"==== Cluster neustarten mit allen Nodes ====\")\n",
    "    worker_pids = setup_spark_cluster(\n",
    "        worker_count=WORKER_INSTANCES,\n",
    "        worker_cores=WORKER_CORES,\n",
    "        worker_memory=WORKER_MEMORY\n",
    "    )\n",
    "\n",
    "    spark.stop()\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # Versuch, den SparkContext zu erstellen\n",
    "    try:\n",
    "        sc = spark.sparkContext  # Verwende den SparkContext aus der SparkSession\n",
    "        print(\"SparkContext erfolgreich erstellt.\")\n",
    "        print(f\"Spark läuft auf Master: {sc.master}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen des SparkContext: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skalierbarkeit**\n",
    "- Definiert verschiedene Datenmengen (10%, 20%, 50%, 100%) zur Analyse der Verarbeitungseffizienz.\n",
    "- Misst die Ausführungszeit der Funktion `analyze_flight_movements` für jede Datenmenge und subtrahiert die Basiszeit.\n",
    "- Speicherauslastung: Erfasst die RAM-Auslastung im Cluster unmittelbar der Verarbeitung.\n",
    "- CPU-Auslastung: Misst die CPU-Auslastung im Cluster unmittelbar nach der Verarbeitung.\n",
    "  \n",
    "- Datenvisualisierung: Erstellt ein Diagramm mit Matplotlib, um die gesammelten Leistungsmetriken in Abhängigkeit von der verarbeiteten Datenmenge darzustellen:\n",
    "  1. Verarbeitungszeit vs. Datenmenge (%)\n",
    "  2. RAM-Auslastung vs. Datenmenge (%)\n",
    "  3. CPU-Auslastung vs. Datenmenge (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Definition der zu verarbeitenden Datenmenge\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "    \n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktionen aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    vehicle_rdd = restructure_flight_movements(dataRDD)\n",
    "    result, aircraft_raw_rdd, aircraftRDD = analyze_aircraft_data(aircraft_data_path)\n",
    "    combined_rdd = combine_rdds(vehicle_rdd, aircraft_raw_rdd)\n",
    "    combined_analyzed_rdd, result = analyze_combined_rdd(combined_rdd, percentage)\n",
    "    \n",
    "    # Cluster-Metriken abrufen\n",
    "    total_memory, total_cpu = get_executor_metrics()\n",
    "\n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Daten für den Plot sammeln\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time - start_time)  # Verarbeitungszeit\n",
    "    memory_data.append(total_memory)  # Speicherauslastung in %\n",
    "    cpu_data.append(total_cpu)  # CPU-Auslastung in %\n",
    "  \n",
    "# Ergebnisse plotten\n",
    "plot_results(x_data, y_time, memory_data, cpu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Analyse - Heatmap der Bewegungsdaten\n",
    "\n",
    "- Laufzeit:\n",
    "  - Führt die Funktionen `analyze_flight_movements`, `process_and_reduce` und `create_heatmap` mit einem sehr kleinen Datenanteil (`0.0001%`) aus.\n",
    "  - Misst die dafür benötigte Zeit (`base_time`), um eine Basiszeit zu bestimmen.\n",
    "  - Führt dieselben Funktionen mit `100%` der Daten aus.\n",
    "  - Misst die gesamte Ausführungszeit und subtrahiert die zuvor ermittelte Basiszeit, um die tatsächliche Verarbeitungszeit zu berechnen.\n",
    "  \n",
    "- Datenverteilung: Analysiert die Verteilung der Daten im reduzierten RDD über die verschiedenen Partitionen mithilfe der Funktion `analyze_partition_distribution`.\n",
    "  \n",
    "- Ausgabe:\n",
    "  - Gibt die berechnete Laufzeit für die Verarbeitung der vollständigen Datenmenge sowie die Ergebnisse der Datenverteilungsanalyse aus.\n",
    "  - Erstellt und zeigt eine Heatmap basierend auf den reduzierten Koordinaten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Laufzeit**\n",
    "- Durchführung der Funktion mit einer minimalen Teilmenge der Daten, um eine Basismessung der benötigten Zeit zu erhalten.\n",
    "- Ausgabe der Laufzeit bei der Verarbeitung aller Daten, abzüglich der Basiszeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Basiszeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=0.0001)\n",
    "reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "map = create_heatmap(reduced_rdd)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "# Basiszeit errechenen\n",
    "base_time = end_time-start_time\n",
    "\n",
    "\n",
    "# Laufzeit\n",
    "\n",
    "# Startzeit messen\n",
    "start_time = time.time()\n",
    "\n",
    "# Funktionen aufrufen\n",
    "result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path)\n",
    "reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "print(result)\n",
    "map = create_heatmap(reduced_rdd)\n",
    "\n",
    "# Endzeit messen\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Die Funktion hat für 100% der Daten {(end_time - start_time - base_time):.4f} Sekunden benötigt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datenverteilung**\n",
    "- Bestimmung der Gesamtzahl der Datenpunkte im Dataset.\n",
    "- Untersuchung der Anzahl der Partitionen sowie der Verteilung der Daten über diese Partitionen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_partition_distribution(reduced_rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fehlertoleranz**\n",
    "- Durchführung einer Analyse in Spark Cluster.\n",
    "- Optional: Händisches beenden (\"killen\") einer Worker Node im Cluster.\n",
    "- Ausgabe eines Protokolls, dass die benötigte Zeit sowie die Ergebnisse der Analyse zeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worker_pids = simulate_worker_failure(process_and_reduce, dataRDD, worker_pids) # Visualisierung findet bei Client statt, somit nicht relevant für Cluster\n",
    "\n",
    "if len(worker_pids) != WORKER_INSTANCES:\n",
    "    print(\"\")\n",
    "    print(\"==== Cluster neustarten mit allen Nodes ====\")\n",
    "    worker_pids = setup_spark_cluster(\n",
    "        worker_count=WORKER_INSTANCES,\n",
    "        worker_cores=WORKER_CORES,\n",
    "        worker_memory=WORKER_MEMORY\n",
    "    )\n",
    "\n",
    "    spark.stop()\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # Versuch, den SparkContext zu erstellen\n",
    "    try:\n",
    "        sc = spark.sparkContext  # Verwende den SparkContext aus der SparkSession\n",
    "        print(\"SparkContext erfolgreich erstellt.\")\n",
    "        print(f\"Spark läuft auf Master: {sc.master}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen des SparkContext: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skalierbarkeit**\n",
    "- Definiert verschiedene Datenmengen (10%, 20%, 50%, 100%) zur Analyse der Verarbeitungseffizienz.\n",
    "- Misst die Ausführungszeit der Funktion `analyze_flight_movements` für jede Datenmenge und subtrahiert die Basiszeit.\n",
    "- Speicherauslastung: Erfasst die RAM-Auslastung im Cluster unmittelbar der Verarbeitung.\n",
    "- CPU-Auslastung: Misst die CPU-Auslastung im Cluster unmittelbar nach der Verarbeitung.\n",
    "  \n",
    "- Datenvisualisierung: Erstellt ein Diagramm mit Matplotlib, um die gesammelten Leistungsmetriken in Abhängigkeit von der verarbeiteten Datenmenge darzustellen:\n",
    "  1. Verarbeitungszeit vs. Datenmenge (%)\n",
    "  2. RAM-Auslastung vs. Datenmenge (%)\n",
    "  3. CPU-Auslastung vs. Datenmenge (%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Skalierbarkeit\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "\n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktionen aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    reduced_rdd, result = process_and_reduce(dataRDD)\n",
    "    map = create_heatmap(reduced_rdd)\n",
    "\n",
    "    # Cluster-Metriken abrufen\n",
    "    total_memory, total_cpu = get_executor_metrics()\n",
    "\n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Daten für den Plot sammeln\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time - start_time)  # Verarbeitungszeit\n",
    "    memory_data.append(total_memory)  # Speicherauslastung in %\n",
    "    cpu_data.append(total_cpu)  # CPU-Auslastung in %\n",
    "\n",
    "# Ergebnisse plotten\n",
    "plot_results(x_data, y_time, memory_data, cpu_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Analyse - Skalierbarkeit bei veränderten Ressourcen\n",
    "\n",
    "- Verdopplung der Hardware Ressourcen im Spark Cluster\n",
    "- Ausführung der Analyse: analyze_flight_movements\n",
    "- Auswertung der Laufzeit im Vergleich zur Durchführung ohne verdoppelte Ressourcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Skalierbarkeit\n",
    "\n",
    "# Definition der zu verarbeitenden Datenmenge\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "# Listen für Plot vorbereiten\n",
    "x_data = []\n",
    "y_time = []\n",
    "memory_data = []\n",
    "cpu_data = []\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten...\")\n",
    "\n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktion aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    \n",
    "    # Cluster-Metriken abrufen\n",
    "    total_memory, total_cpu = get_executor_metrics()\n",
    "\n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Daten für den Plot sammeln\n",
    "    x_data.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time.append(end_time - start_time)  # Verarbeitungszeit\n",
    "    memory_data.append(total_memory)  # Speicherauslastung in %\n",
    "    cpu_data.append(total_cpu)  # CPU-Auslastung in %\n",
    "\n",
    "plot_results(x_data, y_time, memory_data, cpu_data)\n",
    "\n",
    "cleanup_spark_cluster()\n",
    "time.sleep(5)\n",
    "\n",
    "# Ressourcen verdoppeln\n",
    "print(\"\\n=== Verdopple Ressourcen: Anzahl Worker und CPU-Kerne ===\")\n",
    "WORKER_INSTANCES *= 2\n",
    "WORKER_CORES *= 2\n",
    "memory_value = int(WORKER_MEMORY[:-1])  # Entfernt das 'g' und konvertiert den Wert in eine Zahl\n",
    "WORKER_MEMORY = f\"{memory_value * 2}g\"  #\n",
    "\n",
    "# Spark-Cluster mit verdoppelten Ressourcen starten\n",
    "if len(worker_pids) != WORKER_INSTANCES:\n",
    "    print(\"\")\n",
    "    print(\"==== Cluster neustarten mit allen Nodes ====\")\n",
    "    worker_pids = setup_spark_cluster(\n",
    "        worker_count=WORKER_INSTANCES,\n",
    "        worker_cores=WORKER_CORES,\n",
    "        worker_memory=WORKER_MEMORY\n",
    "    )\n",
    "\n",
    "    spark.stop()\n",
    "    spark = create_spark_session()\n",
    "\n",
    "    # Versuch, den SparkContext zu erstellen\n",
    "    try:\n",
    "        sc = spark.sparkContext  # Verwende den SparkContext aus der SparkSession\n",
    "        print(\"SparkContext erfolgreich erstellt.\")\n",
    "        print(f\"Spark läuft auf Master: {sc.master}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fehler beim Erstellen des SparkContext: {e}\")\n",
    "\n",
    "# Skalierbarkeit nach Verdopplung der Ressourcen messen\n",
    "x_data_doubled = []\n",
    "y_time_doubled = []\n",
    "memory_data_doubled = []\n",
    "cpu_data_doubled = []\n",
    "\n",
    "percentages = [0.1, 0.2, 0.5, 1.0]  # 10%, 20%, 50%, 100%\n",
    "\n",
    "\n",
    "for percentage in percentages:\n",
    "    print(f\"\\nVerarbeite {percentage*100}% der Daten mit verdoppelten Ressourcen...\")\n",
    "\n",
    "    # Startzeit messen\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Funktion aufrufen\n",
    "    result_text, map_result, dataRDD = analyze_flight_movements(flight_data_path, percentage=percentage)\n",
    "    \n",
    "    # Cluster-Metriken abrufen\n",
    "    total_memory, total_cpu = get_executor_metrics()\n",
    "\n",
    "    # Endzeit messen\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Daten für den Plot sammeln\n",
    "    x_data_doubled.append(percentage * 100)  # Prozentuale Datenmenge\n",
    "    y_time_doubled.append(end_time - start_time)  # Verarbeitungszeit\n",
    "    memory_data.append(total_memory)  # Speicherauslastung in %\n",
    "    cpu_data.append(total_cpu)  # CPU-Auslastung in %\n",
    "\n",
    "# Ergebnisse nach Verdopplung der Ressourcen plotten\n",
    "plot_results(x_data_doubled, y_time_doubled, memory_data_doubled, cpu_data_doubled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "================================================================================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Experimente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Vorverarbeitung der Datenquelle\n",
    "- Nur notwendig für Vorverarbeitung von Bewegungsdaten zur Umwandlung in notwendiges Datenformat\n",
    "- Laden: Liest eine Pipe-getrennte CSV-Datei (`data.csv`).\n",
    "- Spalten auswählen und umbenennen**: Wählt die Spalten `icao24`, `time`, `latitude`, `longitude`, `onground` aus und benennt sie um.\n",
    "- Daten konvertieren**:\n",
    "    - Zeit: Wandelt Unix-Timestamps in lesbares Format um.\n",
    "    - Status: Konvertiert `onground` von `true`/`false` zu `1`/`0`.\n",
    "- Speichern: Speichert die bereinigten Daten in `processed_data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from datetime import datetime\n",
    "\n",
    "# CSV-Datei laden, wobei das Pipe-Zeichen als Trennzeichen angegeben wird\n",
    "#csv_file_path = \"data.csv\"  # Ersetze dies mit dem Pfad zu deiner CSV-Datei\n",
    "#df = pd.read_csv(csv_file_path, sep='|')  # Verwende '|' als Trennzeichen\n",
    "\n",
    "# Angenommene Spaltennummern: time = 0, icao24 = 1, lat = 2, lon = 3, onground = 4\n",
    "#column_indices = [1, 0, 2, 3, 4]  # Die Spalten in der gewünschten Reihenfolge auswählen\n",
    "#df_selected = df.iloc[:, column_indices]\n",
    "\n",
    "# Umbenennen der Spalten\n",
    "#df_selected.columns = ['time', 'icao24', 'latitude', 'longitude', 'onground']\n",
    "\n",
    "# Funktion zur Konvertierung des Unix-Timestamps\n",
    "#def convert_timestamp(unix_timestamp):\n",
    "#    try:\n",
    "#        return datetime.utcfromtimestamp(int(unix_timestamp)).strftime('%Y/%m/%d %H:%M:%S')\n",
    "#    except ValueError:\n",
    "#        return None\n",
    "\n",
    "# Konvertiere die 'time'-Spalte\n",
    "#df_selected['time'] = df_selected['time'].apply(convert_timestamp)\n",
    "\n",
    "# Umwandlung der 'onground'-Spalte, 'true' -> 1 und 'false' -> 0\n",
    "#df_selected['onground'] = df_selected['onground'].apply(lambda x: 1 if str(x).strip().lower() == 'true' else 0)\n",
    "\n",
    "# Sicherstellen, dass 'time' an erster Stelle bleibt\n",
    "#df_selected = df_selected[['time', 'icao24', 'latitude', 'longitude', 'onground']]\n",
    "\n",
    "# Speichere das Ergebnis als neue CSV-Datei\n",
    "#df_selected.to_csv(\"processed_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 POC zu Parquet\n",
    "\n",
    "- Liest ein Parquet-File (`bigDataSet_v2.parquet`) in einen Spark DataFrame.  \n",
    "- Zeigt die ersten 3 Zeilen (`df.show(3)`) und druckt Schema und Spaltennamen.  \n",
    "- Wandelt den DataFrame in ein RDD um (`df.rdd`) und gibt die ersten 3 Einträge aus (`rdd.take(3)`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Datenverteilung im CLuster\n",
    "# def analyze_partition_distribution(rdd):\n",
    "#     \"\"\"\n",
    "#     Analysiert die Verteilung der Daten in einem RDD über die Partitionen.\n",
    "\n",
    "#     Diese Funktion berechnet die Größe jeder Partition in einem RDD und gibt eine Übersicht über \n",
    "#     die Anzahl der Datensätze in jeder Partition sowie die Gesamtzahl der Datensätze und Partitionen zurück.\n",
    "\n",
    "#     :param rdd: Das RDD, das analysiert werden soll.\n",
    "\n",
    "#     :return: Ein Tuple bestehend aus:\n",
    "#              - Der Anzahl der Datensätze im RDD\n",
    "#              - Der Anzahl der Partitionen im RDD\n",
    "#              - Eine formatierte String-Ausgabe der Verteilung der Daten auf die Partitionen\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Funktion, um die Größe jeder Partition zu berechnen\n",
    "#     def partition_sizes(index, iterator):\n",
    "#         yield index, sum(1 for _ in iterator)\n",
    "\n",
    "#    # Daten je Partition sammeln\n",
    "#     partition_info = rdd.mapPartitionsWithIndex(partition_sizes).collect()\n",
    "\n",
    "#     # Anzahl der Partitionen ermitteln\n",
    "#     num_partitions = rdd.getNumPartitions()\n",
    "\n",
    "#     #Ergebnisse formatieren\n",
    "#     number_data = f\"Anzahl der Datensätze: {rdd.count()}\\n\"\n",
    "#     number_partition = f\"Anzahl der Partitionen: {num_partitions}\\n\"\n",
    "#     result = \"Datenverteilung auf Partitionen:\\n\"\n",
    "\n",
    "#     # Ausgabe der Verteilung pro Partition\n",
    "#     for partition, size in partition_info:\n",
    "#         result += f\"Partition {partition}: {size} Datensätze\\n\"\n",
    "\n",
    "#     return number_data, number_partition, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def get_system_info():\n",
    "#     \"\"\"\n",
    "#     Ermittelt und gibt eine Übersicht über die Systeminformationen zurück.\n",
    "\n",
    "#     Diese Funktion sammelt und gibt verschiedene Systeminformationen wie CPU- und RAM-Daten zurück. \n",
    "#     Dazu gehören die Anzahl der physischen und logischen CPUs, die CPU-Frequenz, die CPU-Auslastung sowie \n",
    "#     Details zum RAM, wie der verfügbare, genutzte und gesamte Speicher sowie die RAM-Auslastung.\n",
    "\n",
    "#     :return: Ein String, der die Systeminformationen, einschließlich der CPU- und RAM-Daten, formatiert darstellt.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # CPU-Informationen\n",
    "#     cpu_count = psutil.cpu_count(logical=False)  # Anzahl physikalischer CPU-Kerne\n",
    "#     logical_cpu_count = psutil.cpu_count(logical=True)  # Anzahl logischer CPUs (mit Hyper-Threading)\n",
    "#     cpu_freq = psutil.cpu_freq()  # CPU Frequenz\n",
    "#     cpu_percent = psutil.cpu_percent(interval=1)  # CPU-Auslastung in Prozent\n",
    "\n",
    "#     # RAM-Informationen\n",
    "#     virtual_memory = psutil.virtual_memory()  # Virtueller Speicher (RAM)\n",
    "#     total_memory = virtual_memory.total  # Gesamtgröße des RAM\n",
    "#     available_memory = virtual_memory.available  # Verfügbarer RAM\n",
    "#     used_memory = virtual_memory.used  # Genutzter RAM\n",
    "#     memory_percent = virtual_memory.percent  # Prozentsatz des verwendeten RAM\n",
    "\n",
    "\n",
    "#     # Ausgabe der gesammelten Informationen\n",
    "#     result = \"=\" * 50\n",
    "#     result += \"System Informationsübersicht\"\n",
    "#     result += \"=\" * 50\\\n",
    "\n",
    "#     result += \"\\nCPU Infos:\\n\"\n",
    "#     result += f\"  - Anzahl der physischen CPU-Kerne: {cpu_count}\\n\"\n",
    "#     result += f\"  - Anzahl der logischen CPU-Kerne (inkl. Hyper-Threading): {logical_cpu_count}\\n\"\n",
    "#     result += f\"  - Aktuelle CPU Frequenz: {cpu_freq.current} MHz\\n\"\n",
    "#     result += f\"  - Aktuelle CPU Auslastung: {cpu_percent}%\\n\"\n",
    "\n",
    "#     result += \"\\nRAM Infos:\\n\"\n",
    "#     result += f\"  - Gesamter RAM: {total_memory / (1024 ** 3):.2f} GB\\n\"\n",
    "#     result += f\"  - Verfügbarer RAM: {available_memory / (1024 ** 3):.2f} GB\\n\"\n",
    "#     result += f\"  - Genutzter RAM: {used_memory / (1024 ** 3):.2f} GB\\n\"\n",
    "#     result += f\"  - RAM Auslastung: {memory_percent}%\"\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Abschließende Diskussion\n",
    "\n",
    "### 5.1 Erkentnisse aus dem Projekt\n",
    "\n",
    "\n",
    "### 5.2 Learnings aus dem Projekt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
